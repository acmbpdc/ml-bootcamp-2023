{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Machine Learning Bootcamp - 2023 \ud83d\udcca","text":"<p>This repository contains the resources for the Machine Learning Bootcamp - 2023 organized by ACM.</p>"},{"location":"#overview","title":"Overview","text":"<p>This bootcamp will provide its audience the core foundation in this amazing field of Data Science and Machine Learning, with Python.</p>"},{"location":"#sessions","title":"Sessions","text":"Session Topic 1 Data Preprocessing 2 Model Building I 3 Model Building II 4 Evaluating and Tuning"},{"location":"#instructors","title":"Instructors","text":"<p>This workshop is conducted by:</p> <ul> <li>Abshar Mohammed Aslam</li> <li>Anurag Kumar Jha</li> <li>Giri Prasad</li> <li>Kayan Irani</li> <li>Mohamed Firas Adil</li> <li>Sanvit Katrekar</li> </ul>"},{"location":"01-data-preprocessing/docs/","title":"Data Preprocessing","text":"In\u00a0[1]: Copied! <pre>! pip install pandas\n! pip install numpy\n</pre> ! pip install pandas ! pip install numpy <pre>Requirement already satisfied: pandas in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.3.0)\nRequirement already satisfied: numpy&gt;=1.17.3 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.19.5)\nRequirement already satisfied: pytz&gt;=2017.3 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2021.1)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.8.2)\nRequirement already satisfied: six&gt;=1.5 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n</pre> <pre>\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n</pre> <pre>Requirement already satisfied: numpy in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.19.5)</pre> <pre>\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n</pre> <pre>\n</pre> In\u00a0[2]: Copied! <pre>print(\"Heya World\") # Will print to console, this is a comment\n</pre> print(\"Heya World\") # Will print to console, this is a comment <pre>Heya World\n</pre> <p>Do note that python does not use curly braces unlike most languages, it doesn't even need semicolons at the end of each statement;</p> <p>Instead it solely relies on indentation.</p> In\u00a0[3]: Copied! <pre>a = 1+1 # basic arithmethic\nif a == 2:\n    print(\"Maths is real\")\nelif a == 1: # elif is how you write else if in python\n    print(\"How??\")\nelse:\n    print(\"What??\")\n</pre> a = 1+1 # basic arithmethic if a == 2:     print(\"Maths is real\") elif a == 1: # elif is how you write else if in python     print(\"How??\") else:     print(\"What??\") <pre>Maths is real\n</pre> In\u00a0[4]: Copied! <pre>for i in range(10): # loops 10 times from 0 to 9\n    print(i)\n\n###\n\ni=0\nwhile i&lt;10: # Equaivalent while loop of the above for\n    print(i) \n    i += 1 # Abbreviated means of addition can also use -= /= or */\n</pre> for i in range(10): # loops 10 times from 0 to 9     print(i)  ###  i=0 while i&lt;10: # Equaivalent while loop of the above for     print(i)      i += 1 # Abbreviated means of addition can also use -= /= or */ <pre>0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n</pre> In\u00a0[5]: Copied! <pre>def ml_function(): # How to define a function\n    print(\"It is 2023\")\n\nml_function() # How to call a function\n</pre> def ml_function(): # How to define a function     print(\"It is 2023\")  ml_function() # How to call a function <pre>It is 2023\n</pre> In\u00a0[6]: Copied! <pre>import math\n\nprint(math.pi)\n</pre> import math  print(math.pi) <pre>3.141592653589793\n</pre> <p>How does this work?</p> <p>When we write <code>import math</code>, Python brings in all the code written under the math module into our program, and then we access math.PI which is a constant in the module. You can always learn more about any module by referring to the documentation.</p> <p>The libraries that are important to us, are those that do things related to Machine Learning, initially we will see how to handle data, with the help of a library known as Pandas.</p> In\u00a0[7]: Copied! <pre>import pandas as pd\n\ndata = pd.read_csv('data.csv')\ndata.head()\n# data.tail()\n</pre> import pandas as pd  data = pd.read_csv('data.csv') data.head() # data.tail() Out[7]: Country Age Salary Purchased 0 France 44.0 72000.0 No 1 Spain 27.0 48000.0 Yes 2 Germany 30.0 54000.0 No 3 Spain 38.0 61000.0 No 4 Germany 40.0 NaN Yes In\u00a0[8]: Copied! <pre>data.shape\n</pre> data.shape Out[8]: <pre>(10, 4)</pre> In\u00a0[9]: Copied! <pre>data.columns\n</pre> data.columns Out[9]: <pre>Index(['Country', 'Age', 'Salary', 'Purchased'], dtype='object')</pre> In\u00a0[10]: Copied! <pre>data.loc[2:8] # Reads this as a label, since our index is numeric lookss same as iloc\n</pre> data.loc[2:8] # Reads this as a label, since our index is numeric lookss same as iloc  Out[10]: Country Age Salary Purchased 2 Germany 30.0 54000.0 No 3 Spain 38.0 61000.0 No 4 Germany 40.0 NaN Yes 5 France 35.0 58000.0 Yes 6 Spain NaN 52000.0 No 7 France 48.0 79000.0 Yes 8 Germany 50.0 83000.0 No In\u00a0[11]: Copied! <pre>data.loc[data['Salary']&gt;70000] # loc allows us conditionals\n</pre> data.loc[data['Salary']&gt;70000] # loc allows us conditionals Out[11]: Country Age Salary Purchased 0 France 44.0 72000.0 No 7 France 48.0 79000.0 Yes 8 Germany 50.0 83000.0 No In\u00a0[12]: Copied! <pre>data.loc[(data['Purchased'].str.contains('No')) &amp; ( data['Salary']&gt;70000)] # Grouping conditionals is also possible\n</pre> data.loc[(data['Purchased'].str.contains('No')) &amp; ( data['Salary']&gt;70000)] # Grouping conditionals is also possible Out[12]: Country Age Salary Purchased 0 France 44.0 72000.0 No 8 Germany 50.0 83000.0 No In\u00a0[13]: Copied! <pre>data.iloc[2:8]\n</pre> data.iloc[2:8] Out[13]: Country Age Salary Purchased 2 Germany 30.0 54000.0 No 3 Spain 38.0 61000.0 No 4 Germany 40.0 NaN Yes 5 France 35.0 58000.0 Yes 6 Spain NaN 52000.0 No 7 France 48.0 79000.0 Yes In\u00a0[14]: Copied! <pre>data.iloc[2:8, 2:5]\n</pre> data.iloc[2:8, 2:5] Out[14]: Salary Purchased 2 54000.0 No 3 61000.0 No 4 NaN Yes 5 58000.0 Yes 6 52000.0 No 7 79000.0 Yes In\u00a0[15]: Copied! <pre>X = data.iloc[:, 1:].values\nprint(X)\n</pre> X = data.iloc[:, 1:].values print(X) <pre>[[44.0 72000.0 'No']\n [27.0 48000.0 'Yes']\n [30.0 54000.0 'No']\n [38.0 61000.0 'No']\n [40.0 nan 'Yes']\n [35.0 58000.0 'Yes']\n [nan 52000.0 'No']\n [48.0 79000.0 'Yes']\n [50.0 83000.0 'No']\n [37.0 67000.0 'Yes']]\n</pre> In\u00a0[16]: Copied! <pre>import IPython.display as ipd\nimport numpy as np\n# Loading data\ndata = pd.read_csv(\"Train.csv\")\ndata.head() # By default gives only 5 entries, Has a max limit of 50, accepts parameter upto 50\n# data.tail()\n</pre> import IPython.display as ipd import numpy as np # Loading data data = pd.read_csv(\"Train.csv\") data.head() # By default gives only 5 entries, Has a max limit of 50, accepts parameter upto 50 # data.tail() Out[16]: Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales 0 FDA15 9.30 Low Fat 0.016047 Dairy 249.8092 OUT049 1999 Medium Tier 1 Supermarket Type1 3735.1380 1 DRC01 5.92 Regular 0.019278 Soft Drinks 48.2692 OUT018 2009 Medium Tier 3 Supermarket Type2 443.4228 2 FDN15 17.50 Low Fat 0.016760 Meat 141.6180 OUT049 1999 Medium Tier 1 Supermarket Type1 2097.2700 3 FDX07 19.20 Regular 0.000000 Fruits and Vegetables 182.0950 OUT010 1998 NaN Tier 3 Grocery Store 732.3800 4 NCD19 8.93 Low Fat 0.000000 Household 53.8614 OUT013 1987 High Tier 3 Supermarket Type1 994.7052 In\u00a0[17]: Copied! <pre>data[\"Item_Type\"].unique()\n</pre> data[\"Item_Type\"].unique() Out[17]: <pre>array(['Dairy', 'Soft Drinks', 'Meat', 'Fruits and Vegetables',\n       'Household', 'Baking Goods', 'Snack Foods', 'Frozen Foods',\n       'Breakfast', 'Health and Hygiene', 'Hard Drinks', 'Canned',\n       'Breads', 'Starchy Foods', 'Others', 'Seafood'], dtype=object)</pre> In\u00a0[18]: Copied! <pre>data[\"Item_Type\"].value_counts()\n# ipd.display(data[\"Item_Type\"].count())\n</pre> data[\"Item_Type\"].value_counts() # ipd.display(data[\"Item_Type\"].count()) Out[18]: <pre>Fruits and Vegetables    1232\nSnack Foods              1200\nHousehold                 910\nFrozen Foods              856\nDairy                     682\nCanned                    649\nBaking Goods              648\nHealth and Hygiene        520\nSoft Drinks               445\nMeat                      425\nBreads                    251\nHard Drinks               214\nOthers                    169\nStarchy Foods             148\nBreakfast                 110\nSeafood                    64\nName: Item_Type, dtype: int64</pre> In\u00a0[19]: Copied! <pre>data[\"Item_Fat_Content\"].unique()\n</pre> data[\"Item_Fat_Content\"].unique()  Out[19]: <pre>array(['Low Fat', 'Regular', 'low fat', 'LF', 'reg'], dtype=object)</pre> In\u00a0[20]: Copied! <pre>data[\"Item_Fat_Content\"].value_counts()\n</pre> data[\"Item_Fat_Content\"].value_counts() Out[20]: <pre>Low Fat    5089\nRegular    2889\nLF          316\nreg         117\nlow fat     112\nName: Item_Fat_Content, dtype: int64</pre> <p>To deal with missing values we could replace each manually or we could python in-built string processing library by:</p> <ol> <li>Converting all variables to lower-case, this takes care of capitalization inconsistencies</li> <li>Converting all alternate labels to a singular label, lf and ref to low fat and regular</li> </ol> In\u00a0[21]: Copied! <pre>dat_copy = data.copy() # Making a shallow copy to avoid any conversion issues\ndat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].str.lower()\nipd.display(dat_copy[\"Item_Fat_Content\"].unique())\n\n\n# dat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].replace(\"lf\",\"low fat\")\ndat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].replace({\"lf\":\"low fat\", \"reg\": \"regular\"})\ndat_copy[\"Item_Fat_Content\"].unique()\n\n# dat_copy[\"Item_Fat_Content\"].value_counts()\n</pre> dat_copy = data.copy() # Making a shallow copy to avoid any conversion issues dat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].str.lower() ipd.display(dat_copy[\"Item_Fat_Content\"].unique())   # dat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].replace(\"lf\",\"low fat\") dat_copy[\"Item_Fat_Content\"] = dat_copy[\"Item_Fat_Content\"].replace({\"lf\":\"low fat\", \"reg\": \"regular\"}) dat_copy[\"Item_Fat_Content\"].unique()  # dat_copy[\"Item_Fat_Content\"].value_counts() <pre>array(['low fat', 'regular', 'lf', 'reg'], dtype=object)</pre> Out[21]: <pre>array(['low fat', 'regular'], dtype=object)</pre> In\u00a0[22]: Copied! <pre>data = dat_copy\ndata.head(15)\n</pre> data = dat_copy data.head(15) Out[22]: Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales 0 FDA15 9.300 low fat 0.016047 Dairy 249.8092 OUT049 1999 Medium Tier 1 Supermarket Type1 3735.1380 1 DRC01 5.920 regular 0.019278 Soft Drinks 48.2692 OUT018 2009 Medium Tier 3 Supermarket Type2 443.4228 2 FDN15 17.500 low fat 0.016760 Meat 141.6180 OUT049 1999 Medium Tier 1 Supermarket Type1 2097.2700 3 FDX07 19.200 regular 0.000000 Fruits and Vegetables 182.0950 OUT010 1998 NaN Tier 3 Grocery Store 732.3800 4 NCD19 8.930 low fat 0.000000 Household 53.8614 OUT013 1987 High Tier 3 Supermarket Type1 994.7052 5 FDP36 10.395 regular 0.000000 Baking Goods 51.4008 OUT018 2009 Medium Tier 3 Supermarket Type2 556.6088 6 FDO10 13.650 regular 0.012741 Snack Foods 57.6588 OUT013 1987 High Tier 3 Supermarket Type1 343.5528 7 FDP10 NaN low fat 0.127470 Snack Foods 107.7622 OUT027 1985 Medium Tier 3 Supermarket Type3 4022.7636 8 FDH17 16.200 regular 0.016687 Frozen Foods 96.9726 OUT045 2002 NaN Tier 2 Supermarket Type1 1076.5986 9 FDU28 19.200 regular 0.094450 Frozen Foods 187.8214 OUT017 2007 NaN Tier 2 Supermarket Type1 4710.5350 10 FDY07 11.800 low fat 0.000000 Fruits and Vegetables 45.5402 OUT049 1999 Medium Tier 1 Supermarket Type1 1516.0266 11 FDA03 18.500 regular 0.045464 Dairy 144.1102 OUT046 1997 Small Tier 1 Supermarket Type1 2187.1530 12 FDX32 15.100 regular 0.100014 Fruits and Vegetables 145.4786 OUT049 1999 Medium Tier 1 Supermarket Type1 1589.2646 13 FDS46 17.600 regular 0.047257 Snack Foods 119.6782 OUT046 1997 Small Tier 1 Supermarket Type1 2145.2076 14 FDF32 16.350 low fat 0.068024 Fruits and Vegetables 196.4426 OUT013 1987 High Tier 3 Supermarket Type1 1977.4260 <p>How to deal with missing data?</p> <ol> <li><p>Drop data a. Drop the whole row b. Drop the whole column</p> </li> <li><p>Replace data a. Replace it by mean b. Replace it by frequency c. Replace it based on other functions</p> </li> </ol> In\u00a0[23]: Copied! <pre>data[\"Outlet_Size\"].unique()\n</pre> data[\"Outlet_Size\"].unique()  Out[23]: <pre>array(['Medium', nan, 'High', 'Small'], dtype=object)</pre> <p>Missing data can also be found by <code>isnull()</code> or <code>isna()</code> functions.</p> In\u00a0[24]: Copied! <pre>ipd.display(data.isnull().any())\n</pre> ipd.display(data.isnull().any()) <pre>Item_Identifier              False\nItem_Weight                   True\nItem_Fat_Content             False\nItem_Visibility              False\nItem_Type                    False\nItem_MRP                     False\nOutlet_Identifier            False\nOutlet_Establishment_Year    False\nOutlet_Size                   True\nOutlet_Location_Type         False\nOutlet_Type                  False\nItem_Outlet_Sales            False\ndtype: bool</pre> <p>We can figure which of our rows are missing their <code>Item Weights</code> by the following:</p> In\u00a0[25]: Copied! <pre>data[data['Item_Weight'].isnull()] #1462 columns\n</pre> data[data['Item_Weight'].isnull()] #1462 columns Out[25]: Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales 7 FDP10 NaN low fat 0.127470 Snack Foods 107.7622 OUT027 1985 Medium Tier 3 Supermarket Type3 4022.7636 18 DRI11 NaN low fat 0.034238 Hard Drinks 113.2834 OUT027 1985 Medium Tier 3 Supermarket Type3 2303.6680 21 FDW12 NaN regular 0.035400 Baking Goods 144.5444 OUT027 1985 Medium Tier 3 Supermarket Type3 4064.0432 23 FDC37 NaN low fat 0.057557 Baking Goods 107.6938 OUT019 1985 Small Tier 1 Grocery Store 214.3876 29 FDC14 NaN regular 0.072222 Canned 43.6454 OUT019 1985 Small Tier 1 Grocery Store 125.8362 ... ... ... ... ... ... ... ... ... ... ... ... ... 8485 DRK37 NaN low fat 0.043792 Soft Drinks 189.0530 OUT027 1985 Medium Tier 3 Supermarket Type3 6261.8490 8487 DRG13 NaN low fat 0.037006 Soft Drinks 164.7526 OUT027 1985 Medium Tier 3 Supermarket Type3 4111.3150 8488 NCN14 NaN low fat 0.091473 Others 184.6608 OUT027 1985 Medium Tier 3 Supermarket Type3 2756.4120 8490 FDU44 NaN regular 0.102296 Fruits and Vegetables 162.3552 OUT019 1985 Small Tier 1 Grocery Store 487.3656 8504 NCN18 NaN low fat 0.124111 Household 111.7544 OUT027 1985 Medium Tier 3 Supermarket Type3 4138.6128 <p>1463 rows \u00d7 12 columns</p> <p>As <code>Item_Weight</code> is a Ratio (continious) variable we replace it by it's <code>Mean</code></p> In\u00a0[26]: Copied! <pre># Replace Outlet by mode\n# Replace Item_weight by mean\n\navg_item_wt = data[\"Item_Weight\"].astype(\"float\").mean()\n\ndata_cpy = data.copy()\ndata_cpy[\"Item_Weight\"].replace(np.nan, avg_item_wt, inplace=True)\ndata_cpy[\"Item_Weight\"].isnull().any() \n#Maybe assert\n</pre> # Replace Outlet by mode # Replace Item_weight by mean  avg_item_wt = data[\"Item_Weight\"].astype(\"float\").mean()  data_cpy = data.copy() data_cpy[\"Item_Weight\"].replace(np.nan, avg_item_wt, inplace=True) data_cpy[\"Item_Weight\"].isnull().any()  #Maybe assert Out[26]: <pre>False</pre> <p>As <code>Outlet_size</code> is an ordinal variable we replace it by it's <code>Mode</code></p> In\u00a0[27]: Copied! <pre>data_cpy = data.copy()\n\nipd.display(data_cpy[\"Outlet_Size\"].value_counts())\noutlet_mode = data_cpy[\"Outlet_Size\"].mode()\ndata_cpy[\"Outlet_Size\"].replace(np.nan, outlet_mode[0], inplace=True)\n\ndata_cpy[\"Outlet_Size\"].isnull().any()\n# data_cpy[\"Outlet_Size\"].value_counts()\n</pre> data_cpy = data.copy()  ipd.display(data_cpy[\"Outlet_Size\"].value_counts()) outlet_mode = data_cpy[\"Outlet_Size\"].mode() data_cpy[\"Outlet_Size\"].replace(np.nan, outlet_mode[0], inplace=True)  data_cpy[\"Outlet_Size\"].isnull().any() # data_cpy[\"Outlet_Size\"].value_counts() <pre>Medium    2793\nSmall     2388\nHigh       932\nName: Outlet_Size, dtype: int64</pre> Out[27]: <pre>False</pre> In\u00a0[28]: Copied! <pre>data.head()\n</pre> data.head() Out[28]: Item_Identifier Item_Weight Item_Fat_Content Item_Visibility Item_Type Item_MRP Outlet_Identifier Outlet_Establishment_Year Outlet_Size Outlet_Location_Type Outlet_Type Item_Outlet_Sales 0 FDA15 9.30 low fat 0.016047 Dairy 249.8092 OUT049 1999 Medium Tier 1 Supermarket Type1 3735.1380 1 DRC01 5.92 regular 0.019278 Soft Drinks 48.2692 OUT018 2009 Medium Tier 3 Supermarket Type2 443.4228 2 FDN15 17.50 low fat 0.016760 Meat 141.6180 OUT049 1999 Medium Tier 1 Supermarket Type1 2097.2700 3 FDX07 19.20 regular 0.000000 Fruits and Vegetables 182.0950 OUT010 1998 NaN Tier 3 Grocery Store 732.3800 4 NCD19 8.93 low fat 0.000000 Household 53.8614 OUT013 1987 High Tier 3 Supermarket Type1 994.7052 In\u00a0[29]: Copied! <pre>data = data_cpy\nipd.display(data.isnull().any())\n</pre> data = data_cpy ipd.display(data.isnull().any()) <pre>Item_Identifier              False\nItem_Weight                   True\nItem_Fat_Content             False\nItem_Visibility              False\nItem_Type                    False\nItem_MRP                     False\nOutlet_Identifier            False\nOutlet_Establishment_Year    False\nOutlet_Size                  False\nOutlet_Location_Type         False\nOutlet_Type                  False\nItem_Outlet_Sales            False\ndtype: bool</pre> <p>Sometimes outliers can mess up an analysis; you usually don't want a handful of data points to skew the overall results. Let's see an example of income data, with some random billionaire thrown in:</p> In\u00a0[30]: Copied! <pre>%matplotlib inline\nimport numpy as np\n\nincomes = np.random.normal(27000, 15000, 10000)\nincomes = np.append(incomes, [1000000000])\n\nimport matplotlib.pyplot as plt\nplt.hist(incomes, 50)\nplt.show()\n</pre> %matplotlib inline import numpy as np  incomes = np.random.normal(27000, 15000, 10000) incomes = np.append(incomes, [1000000000])  import matplotlib.pyplot as plt plt.hist(incomes, 50) plt.show() <p>That's not very helpful to look at. One billionaire ended up squeezing everybody else into a single line in my histogram. Plus it skewed my mean income significantly:</p> In\u00a0[31]: Copied! <pre>incomes.mean()\n</pre> incomes.mean() Out[31]: <pre>127014.53247334201</pre> <p>It's important to dig into what is causing your outliers, and understand where they are coming from. You also need to think about whether removing them is a valid thing to do, given the spirit of what it is you're trying to analyze. If I know I want to understand more about the incomes of \"typical Americans\", filtering out billionaires seems like a legitimate thing to do.</p> <p>Here's something a little more robust than filtering out billionaires - it filters out anything beyond two standard deviations of the median value in the data set:</p> In\u00a0[32]: Copied! <pre>def reject_outliers(data):\n    u = np.median(data)\n    s = np.std(data)\n    filtered = [e for e in data if (u - 2 * s &lt; e &lt; u + 2 * s)]\n    return filtered\n\nfiltered = reject_outliers(incomes)\n\nplt.hist(filtered, 50)\nplt.show()\n</pre> def reject_outliers(data):     u = np.median(data)     s = np.std(data)     filtered = [e for e in data if (u - 2 * s &lt; e &lt; u + 2 * s)]     return filtered  filtered = reject_outliers(incomes)  plt.hist(filtered, 50) plt.show() <p>That looks better. And, our mean is more, well, meangingful now as well:</p> In\u00a0[33]: Copied! <pre>np.mean(filtered)\n</pre> np.mean(filtered) Out[33]: <pre>27027.233926589364</pre> <p>Instead of a single outlier, add several randomly-generated outliers to the data. Experiment with different values of the multiple of the standard deviation to identify outliers, and see what effect it has on the final results.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[34]: Copied! <pre>%matplotlib inline\nfrom scipy.stats import norm\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx = np.arange(-3, 3, 0.01)\n\nplt.plot(x, norm.pdf(x))\nplt.show()\n</pre> %matplotlib inline from scipy.stats import norm import matplotlib.pyplot as plt import numpy as np  x = np.arange(-3, 3, 0.01)  plt.plot(x, norm.pdf(x)) plt.show() In\u00a0[35]: Copied! <pre>plt.plot(x, norm.pdf(x))\nplt.plot(x, norm.pdf(x, 1.0, 0.5))\nplt.show()\n</pre> plt.plot(x, norm.pdf(x)) plt.plot(x, norm.pdf(x, 1.0, 0.5)) plt.show() In\u00a0[36]: Copied! <pre>plt.plot(x, norm.pdf(x))\nplt.plot(x, norm.pdf(x, 1.0, 0.5))\nplt.savefig('MyPlot.png', format='png')\n</pre> plt.plot(x, norm.pdf(x)) plt.plot(x, norm.pdf(x, 1.0, 0.5)) plt.savefig('MyPlot.png', format='png') In\u00a0[37]: Copied! <pre>axes = plt.axes()\naxes.set_xlim([-5, 5])\naxes.set_ylim([0, 1.0])\naxes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\naxes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\nplt.plot(x, norm.pdf(x))\nplt.plot(x, norm.pdf(x, 1.0, 0.5))\nplt.show()\n</pre> axes = plt.axes() axes.set_xlim([-5, 5]) axes.set_ylim([0, 1.0]) axes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]) axes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) plt.plot(x, norm.pdf(x)) plt.plot(x, norm.pdf(x, 1.0, 0.5)) plt.show() In\u00a0[38]: Copied! <pre>axes = plt.axes()\naxes.set_xlim([-5, 5])\naxes.set_ylim([0, 1.0])\naxes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\naxes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\naxes.grid()\nplt.plot(x, norm.pdf(x))\nplt.plot(x, norm.pdf(x, 1.0, 0.5))\nplt.show()\n</pre> axes = plt.axes() axes.set_xlim([-5, 5]) axes.set_ylim([0, 1.0]) axes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]) axes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) axes.grid() plt.plot(x, norm.pdf(x)) plt.plot(x, norm.pdf(x, 1.0, 0.5)) plt.show() In\u00a0[39]: Copied! <pre>axes = plt.axes()\naxes.set_xlim([-5, 5])\naxes.set_ylim([0, 1.0])\naxes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\naxes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\naxes.grid()\nplt.plot(x, norm.pdf(x), 'b-')\nplt.plot(x, norm.pdf(x, 1.0, 0.5), 'r:')\nplt.show()\n</pre> axes = plt.axes() axes.set_xlim([-5, 5]) axes.set_ylim([0, 1.0]) axes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]) axes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) axes.grid() plt.plot(x, norm.pdf(x), 'b-') plt.plot(x, norm.pdf(x, 1.0, 0.5), 'r:') plt.show() In\u00a0[40]: Copied! <pre>axes = plt.axes()\naxes.set_xlim([-5, 5])\naxes.set_ylim([0, 1.0])\naxes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\naxes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0])\naxes.grid()\nplt.xlabel('Greebles')\nplt.ylabel('Probability')\nplt.plot(x, norm.pdf(x), 'b-')\nplt.plot(x, norm.pdf(x, 1.0, 0.5), 'r:')\nplt.legend(['Sneetches', 'Gacks'], loc=4)\nplt.show()\n</pre> axes = plt.axes() axes.set_xlim([-5, 5]) axes.set_ylim([0, 1.0]) axes.set_xticks([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5]) axes.set_yticks([0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]) axes.grid() plt.xlabel('Greebles') plt.ylabel('Probability') plt.plot(x, norm.pdf(x), 'b-') plt.plot(x, norm.pdf(x, 1.0, 0.5), 'r:') plt.legend(['Sneetches', 'Gacks'], loc=4) plt.show() In\u00a0[41]: Copied! <pre>import logging\nlogging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR) # to stop a font import warning\nplt.xkcd()\n\nfig = plt.figure()\nax = fig.add_subplot(1, 1, 1)\nax.spines['right'].set_color('none')\nax.spines['top'].set_color('none')\nplt.xticks([])\nplt.yticks([])\nax.set_ylim([-30, 10])\n\ndata = np.ones(100)\ndata[70:] -= np.arange(30)\n\nplt.annotate(\n    'THE DAY I REALIZED\\nI COULD EAT ICECREAM\\nWHENEVER I WANTED',\n    xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))\n\nplt.plot(data)\n\nplt.xlabel('time')\nplt.ylabel('my overall health')\n</pre> import logging logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR) # to stop a font import warning plt.xkcd()  fig = plt.figure() ax = fig.add_subplot(1, 1, 1) ax.spines['right'].set_color('none') ax.spines['top'].set_color('none') plt.xticks([]) plt.yticks([]) ax.set_ylim([-30, 10])  data = np.ones(100) data[70:] -= np.arange(30)  plt.annotate(     'THE DAY I REALIZED\\nI COULD EAT ICECREAM\\nWHENEVER I WANTED',     xy=(70, 1), arrowprops=dict(arrowstyle='-&gt;'), xytext=(15, -10))  plt.plot(data)  plt.xlabel('time') plt.ylabel('my overall health') Out[41]: <pre>Text(0, 0.5, 'my overall health')</pre> In\u00a0[42]: Copied! <pre># Remove XKCD mode:\nplt.rcdefaults()\n\nvalues = [12, 55, 4, 32, 14]\ncolors = ['r', 'g', 'b', 'c', 'm']\nexplode = [0, 0, 0.2, 0, 0]\nlabels = ['India', 'United States', 'Russia', 'China', 'Europe']\nplt.pie(values, colors= colors, labels=labels, explode = explode)\nplt.title('Student Locations')\nplt.show()\n</pre> # Remove XKCD mode: plt.rcdefaults()  values = [12, 55, 4, 32, 14] colors = ['r', 'g', 'b', 'c', 'm'] explode = [0, 0, 0.2, 0, 0] labels = ['India', 'United States', 'Russia', 'China', 'Europe'] plt.pie(values, colors= colors, labels=labels, explode = explode) plt.title('Student Locations') plt.show() In\u00a0[43]: Copied! <pre>values = [12, 55, 4, 32, 14]\ncolors = ['r', 'g', 'b', 'c', 'm']\nplt.bar(range(0,5), values, color= colors)\nplt.show()\n</pre> values = [12, 55, 4, 32, 14] colors = ['r', 'g', 'b', 'c', 'm'] plt.bar(range(0,5), values, color= colors) plt.show() In\u00a0[44]: Copied! <pre>from pylab import randn\n\nX = randn(500)\nY = randn(500)\nplt.scatter(X,Y)\nplt.show()\n</pre> from pylab import randn  X = randn(500) Y = randn(500) plt.scatter(X,Y) plt.show() In\u00a0[45]: Copied! <pre>incomes = np.random.normal(27000, 15000, 10000)\nplt.hist(incomes, 50)\nplt.show()\n</pre> incomes = np.random.normal(27000, 15000, 10000) plt.hist(incomes, 50) plt.show() <p>Useful for visualizing the spread &amp; skew of data.</p> <p>The red line represents the median of the data, and the box represents the bounds of the 1st and 3rd quartiles.</p> <p>So, half of the data exists within the box.</p> <p>The dotted-line \"whiskers\" indicate the range of the data - except for outliers, which are plotted outside the whiskers. Outliers are 1.5X or more the interquartile range.</p> <p>This example below creates uniformly distributed random numbers between -40 and 60, plus a few outliers above 100 and below -100:</p> In\u00a0[46]: Copied! <pre>uniformSkewed = np.random.rand(100) * 100 - 40\nhigh_outliers = np.random.rand(10) * 50 + 100\nlow_outliers = np.random.rand(10) * -50 - 100\ndata = np.concatenate((uniformSkewed, high_outliers, low_outliers))\nplt.boxplot(data)\nplt.show()\n</pre> uniformSkewed = np.random.rand(100) * 100 - 40 high_outliers = np.random.rand(10) * 50 + 100 low_outliers = np.random.rand(10) * -50 - 100 data = np.concatenate((uniformSkewed, high_outliers, low_outliers)) plt.boxplot(data) plt.show() <p>Try creating a scatter plot representing random data on age vs. time spent watching TV. Label the axes.</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[47]: Copied! <pre>%matplotlib inline\n\nimport pandas as pd\n\ndf = pd.read_csv(\"http://media.sundog-soft.com/SelfDriving/FuelEfficiency.csv\")\n\ngear_counts = df['# Gears'].value_counts()\n\ngear_counts.plot(kind='bar')\n</pre> %matplotlib inline  import pandas as pd  df = pd.read_csv(\"http://media.sundog-soft.com/SelfDriving/FuelEfficiency.csv\")  gear_counts = df['# Gears'].value_counts()  gear_counts.plot(kind='bar') Out[47]: <pre>&lt;AxesSubplot:&gt;</pre> <p>We can load up Seaborn, and just call set() on it to change matplotlib's default settings to something more visually pleasing.</p> In\u00a0[48]: Copied! <pre>!pip install seaborn\n</pre> !pip install seaborn <pre>Requirement already satisfied: seaborn in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.1)\nRequirement already satisfied: scipy&gt;=1.0 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.7.0)\n</pre> <pre>\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n</pre> <pre>Requirement already satisfied: matplotlib&gt;=2.2 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (3.4.2)\nRequirement already satisfied: numpy&gt;=1.15 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.19.5)\nRequirement already satisfied: pandas&gt;=0.23 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.3.0)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (0.10.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (9.0.1)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (1.3.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.4.7)\nRequirement already satisfied: pytz&gt;=2017.3 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas&gt;=0.23-&gt;seaborn) (2021.1)\nRequirement already satisfied: six in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=2.2-&gt;seaborn) (1.15.0)\n</pre> In\u00a0[49]: Copied! <pre>!pip install seaborn\n</pre> !pip install seaborn <pre>Requirement already satisfied: seaborn in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.1)\nRequirement already satisfied: pandas&gt;=0.23 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.3.0)\nRequirement already satisfied: numpy&gt;=1.15 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.19.5)\nRequirement already satisfied: matplotlib&gt;=2.2 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (3.4.2)\nRequirement already satisfied: scipy&gt;=1.0 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from seaborn) (1.7.0)\nRequirement already satisfied: pillow&gt;=6.2.0 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (9.0.1)\nRequirement already satisfied: python-dateutil&gt;=2.7 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.8.2)\nRequirement already satisfied: pyparsing&gt;=2.2.1 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (2.4.7)\nRequirement already satisfied: cycler&gt;=0.10 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (0.10.0)\nRequirement already satisfied: kiwisolver&gt;=1.0.1 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from matplotlib&gt;=2.2-&gt;seaborn) (1.3.1)\nRequirement already satisfied: pytz&gt;=2017.3 in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas&gt;=0.23-&gt;seaborn) (2021.1)\nRequirement already satisfied: six in c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from cycler&gt;=0.10-&gt;matplotlib&gt;=2.2-&gt;seaborn) (1.15.0)\n</pre> <pre>\n[notice] A new release of pip available: 22.3.1 -&gt; 23.0.1\n[notice] To update, run: python.exe -m pip install --upgrade pip\n</pre> In\u00a0[50]: Copied! <pre>import seaborn as sns\nsns.set()\n</pre> import seaborn as sns sns.set() <p>Now if we do the same plot command, it's a little more modern looking. Matplotlib is based on Matplot, and its visualization defaults are frankly showing their age.</p> In\u00a0[51]: Copied! <pre>gear_counts.plot(kind='bar')\n</pre> gear_counts.plot(kind='bar') Out[51]: <pre>&lt;AxesSubplot:&gt;</pre> <p>Let's take a closer look at the data we're dealing with.</p> In\u00a0[52]: Copied! <pre>df.head()\n</pre> df.head() Out[52]: Mfr Name Carline Eng Displ Cylinders Transmission CityMPG HwyMPG CombMPG # Gears 0 aston martin Vantage V8 4.0 8 Auto(S8) 18 25 21 8 1 Volkswagen Group of Chiron 8.0 16 Auto(AM-S7) 9 14 11 7 2 General Motors CORVETTE 6.2 8 Auto(S8) 12 20 15 8 3 General Motors CORVETTE 6.2 8 Auto(S8) 15 25 18 8 4 General Motors CORVETTE 6.2 8 Auto(S8) 14 23 17 8 <p>Seaborn includes many types of plots that matplotlib doens't offer. For example, \"distplot\" can be used to plot a histogram together with a smooth distribution of that histogram overlaid on it. Let's plot the distribution of MPG values on the vehicles in this database as an example:</p> In\u00a0[53]: Copied! <pre>sns.distplot(df['CombMPG'])\n</pre> sns.distplot(df['CombMPG']) <pre>c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n  warnings.warn(msg, FutureWarning)\n</pre> Out[53]: <pre>&lt;AxesSubplot:xlabel='CombMPG', ylabel='Density'&gt;</pre> <p>Something you encounter pretty often is a \"pair plot\" from Seaborn. This lets you visualize plots of every combination of various attributes together, so you can look for interesting patterns between features.</p> <p>As an example, let's classify cars by how many cylinders are in their engines, and look for relationships between cylinders, city MPG rating, Highway MPG rating, and combined MPG rating.</p> In\u00a0[54]: Copied! <pre>df2 = df[['Cylinders', 'CityMPG', 'HwyMPG', 'CombMPG']]\ndf2.head()\n</pre> df2 = df[['Cylinders', 'CityMPG', 'HwyMPG', 'CombMPG']] df2.head() Out[54]: Cylinders CityMPG HwyMPG CombMPG 0 8 18 25 21 1 16 9 14 11 2 8 12 20 15 3 8 15 25 18 4 8 14 23 17 In\u00a0[55]: Copied! <pre>sns.pairplot(df2, height=2.5);  #Seaborn currently has a bug with the hue parameter so we've omitted it\n</pre> sns.pairplot(df2, height=2.5);  #Seaborn currently has a bug with the hue parameter so we've omitted it <p>By studying the results above, you can see there is a relationship between number of cylinders and MPG, but MPG for 4-cylinder vehicles ranges really widely. There also appears to be a good linear relationship between the different ways of measuring MPG values, until you get into the higher MPG ratings.</p> <p>Seaborn 1.9 also includes \"scatterplot\", which is exactly what it sounds like. It plots individual data points across two axes of your choosing, so you can see how your data is distributed across those dimensions.</p> In\u00a0[56]: Copied! <pre>sns.scatterplot(x=\"Eng Displ\", y=\"CombMPG\", data=df)\n</pre> sns.scatterplot(x=\"Eng Displ\", y=\"CombMPG\", data=df) Out[56]: <pre>&lt;AxesSubplot:xlabel='Eng Displ', ylabel='CombMPG'&gt;</pre> <p>Seaborn also offers a \"jointplot\", which combines a scatterplot with histograms on both axes. This lets you visualize both the individual data points and the distribution across both dimensions at the same time.</p> In\u00a0[57]: Copied! <pre>sns.jointplot(x=\"Eng Displ\", y=\"CombMPG\", data=df)\n</pre> sns.jointplot(x=\"Eng Displ\", y=\"CombMPG\", data=df) Out[57]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1c602b2d850&gt;</pre> <p>The \"lmplot\" is a scatterplot, but with a linear regression line computed and overlaid onto the data.</p> In\u00a0[58]: Copied! <pre>sns.lmplot(x=\"Eng Displ\", y=\"CombMPG\", data=df)\n</pre> sns.lmplot(x=\"Eng Displ\", y=\"CombMPG\", data=df) Out[58]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x1c603de5700&gt;</pre> <p>Next, let's look at a \"box plot.\" This is what's called a \"box and whiskers\" plot, which is useful for visualizing typical values for a given category without getting distracted by outliers. Each box represents the range between the first and third quartiles of the data, with a line representing the median value. The \"whiskers\" that extend from the box represent the spread of the remainder of the data, apart from clear outliers that are plotted as individual points outside of the whiskers.</p> <p>As an example, let's look at box plots for each vehicle manufacturer, visualizing the miles-per-gallon ratings across the vehicles they produce. This lets us look at the spread of MPG ratings across all the vehicles each manufacturer offers.</p> <p>There are a lot of manufacturers, so to make the resulting graph readable we'll increase Seaborn's default figure size, and also use set_xticklabels to rotate the labels 45 degrees.</p> In\u00a0[59]: Copied! <pre>sns.set(rc={'figure.figsize':(15,5)})\nax=sns.boxplot(x='Mfr Name', y='CombMPG', data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n</pre> sns.set(rc={'figure.figsize':(15,5)}) ax=sns.boxplot(x='Mfr Name', y='CombMPG', data=df) ax.set_xticklabels(ax.get_xticklabels(),rotation=45) Out[59]: <pre>[Text(0, 0, 'aston martin'),\n Text(1, 0, 'Volkswagen Group of'),\n Text(2, 0, 'General Motors'),\n Text(3, 0, 'Ferrari'),\n Text(4, 0, 'FCA US LLC'),\n Text(5, 0, 'Jaguar Land Rover L'),\n Text(6, 0, 'MAZDA'),\n Text(7, 0, 'Nissan'),\n Text(8, 0, 'Porsche'),\n Text(9, 0, 'BMW'),\n Text(10, 0, 'Subaru'),\n Text(11, 0, 'Toyota'),\n Text(12, 0, 'Ford Motor Company'),\n Text(13, 0, 'Mercedes-Benz'),\n Text(14, 0, 'Honda'),\n Text(15, 0, 'Hyundai'),\n Text(16, 0, 'Rolls-Royce'),\n Text(17, 0, 'Volvo'),\n Text(18, 0, 'Kia'),\n Text(19, 0, 'Maserati'),\n Text(20, 0, 'Mitsubishi Motors Co')]</pre> <p>Another way to visualize the same data is the \"swarm plot.\" Instead of boxes and whiskers, it plots each individual data point - but does so in such way that groups them together based on their distribution. It makes more sense when you look at it:</p> In\u00a0[60]: Copied! <pre>ax=sns.swarmplot(x='Mfr Name', y='CombMPG', data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n</pre> ax=sns.swarmplot(x='Mfr Name', y='CombMPG', data=df) ax.set_xticklabels(ax.get_xticklabels(),rotation=45) <pre>c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 31.4% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 6.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 30.8% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 29.6% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n</pre> Out[60]: <pre>[Text(0, 0, 'aston martin'),\n Text(1, 0, 'Volkswagen Group of'),\n Text(2, 0, 'General Motors'),\n Text(3, 0, 'Ferrari'),\n Text(4, 0, 'FCA US LLC'),\n Text(5, 0, 'Jaguar Land Rover L'),\n Text(6, 0, 'MAZDA'),\n Text(7, 0, 'Nissan'),\n Text(8, 0, 'Porsche'),\n Text(9, 0, 'BMW'),\n Text(10, 0, 'Subaru'),\n Text(11, 0, 'Toyota'),\n Text(12, 0, 'Ford Motor Company'),\n Text(13, 0, 'Mercedes-Benz'),\n Text(14, 0, 'Honda'),\n Text(15, 0, 'Hyundai'),\n Text(16, 0, 'Rolls-Royce'),\n Text(17, 0, 'Volvo'),\n Text(18, 0, 'Kia'),\n Text(19, 0, 'Maserati'),\n Text(20, 0, 'Mitsubishi Motors Co')]</pre> <p>Another tool is the \"count plot.\" This is basically the same thing as a histogram, but for categorical data. It lets you count up how many times each given category on the X axis occurs in your data, and plot it. So for example, we can see that General Motors offers more vehicle models than anyone else, with BMW not far behind.</p> In\u00a0[61]: Copied! <pre>ax=sns.countplot(x='Mfr Name', data=df)\nax.set_xticklabels(ax.get_xticklabels(),rotation=45)\n</pre> ax=sns.countplot(x='Mfr Name', data=df) ax.set_xticklabels(ax.get_xticklabels(),rotation=45) Out[61]: <pre>[Text(0, 0, 'aston martin'),\n Text(1, 0, 'Volkswagen Group of'),\n Text(2, 0, 'General Motors'),\n Text(3, 0, 'Ferrari'),\n Text(4, 0, 'FCA US LLC'),\n Text(5, 0, 'Jaguar Land Rover L'),\n Text(6, 0, 'MAZDA'),\n Text(7, 0, 'Nissan'),\n Text(8, 0, 'Porsche'),\n Text(9, 0, 'BMW'),\n Text(10, 0, 'Subaru'),\n Text(11, 0, 'Toyota'),\n Text(12, 0, 'Ford Motor Company'),\n Text(13, 0, 'Mercedes-Benz'),\n Text(14, 0, 'Honda'),\n Text(15, 0, 'Hyundai'),\n Text(16, 0, 'Rolls-Royce'),\n Text(17, 0, 'Volvo'),\n Text(18, 0, 'Kia'),\n Text(19, 0, 'Maserati'),\n Text(20, 0, 'Mitsubishi Motors Co')]</pre> <p>Finally, let's look at a heat-map in Seaborn. A heat map allows you to plot tabular, 2D data of some sort, with colors representing the individual values in each cell of the 2D table.</p> <p>In this example, we'll create a pivot table from our original dataframe, to create a 2D table that contains the average MPG ratings for every combination of number of cylinders and engine displacement.</p> <p>The resulting heatmap shows all of the engine displacement values along the X axis, and all of the cylinder values along the Y axis. For each cell of the table, the actual average MPG rating for that combination of cylinders and engine displacement is represented not as a number, but as a color that ranges from dark for small values, and light for larger values.</p> <p>And, this does allow you visualize a clear trend where things get progressively darker as we move from the top-left of the graph to the bottom-right. Which makes sense; higher MPG ratings are associated with lower numbers of cylinders, and lower engine displacment values. By the time we get to an 8-liter 16-cylinder engine, the average MPG is at its worst of about 12, represented by the color black.</p> <p>This particular graph has a lot of missing data, but the heatmap deals with that gracefully. A 3-cylinder 8-liter engine simply does not exist!</p> In\u00a0[62]: Copied! <pre>df2 = df.pivot_table(index='Cylinders', columns='Eng Displ', values='CombMPG', aggfunc='mean')\nsns.heatmap(df2)\n</pre> df2 = df.pivot_table(index='Cylinders', columns='Eng Displ', values='CombMPG', aggfunc='mean') sns.heatmap(df2) Out[62]: <pre>&lt;AxesSubplot:xlabel='Eng Displ', ylabel='Cylinders'&gt;</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> <p>My solution is below - no peeking ahead of time!  </p> In\u00a0[63]: Copied! <pre>sns.scatterplot(x='# Gears', y=\"CombMPG\", data=df)\n</pre> sns.scatterplot(x='# Gears', y=\"CombMPG\", data=df) Out[63]: <pre>&lt;AxesSubplot:xlabel='# Gears', ylabel='CombMPG'&gt;</pre> <p>A scatterplot arranges itself into columns when you have ordinal data like the number of gears, but it tells us that there's a pretty wide range of MPG values for each type of gear box, although if you look at where the data points are clustered, you can sort of see a downward trend in MPG as the number of gears increases. But it's subtle.</p> <p>We also see that there's such a thing as a single gear car. These are \"continuously variable transmission\" cars, and we can see they typically have high MPG ratings and are therefore quite efficient.</p> In\u00a0[64]: Copied! <pre>sns.lmplot(x='# Gears', y=\"CombMPG\", data=df)\n</pre> sns.lmplot(x='# Gears', y=\"CombMPG\", data=df) Out[64]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x1c60426aca0&gt;</pre> <p>The \"lmplot\" gives us a linear regression of our data overlaid on the graph, and this makes the overall trend of lower MPG's with more gears apparent. More gears isn't better when it comes to efficiency, it seems. We also see the error bars on that regression line, which tells us this trend is probably real and not just the result of randomness in the samples.</p> In\u00a0[65]: Copied! <pre>sns.jointplot(x='# Gears', y=\"CombMPG\", data=df)\n</pre> sns.jointplot(x='# Gears', y=\"CombMPG\", data=df) Out[65]: <pre>&lt;seaborn.axisgrid.JointGrid at 0x1c602a33370&gt;</pre> <p>The jointplot gives us histograms on each axis, which provides some interesting insights. The most common gear configuration seems to be 8, with 6 closely behind. And MPG ratings seem to roughly follow a bell curve centered at around 22 or so.</p> In\u00a0[66]: Copied! <pre>sns.boxplot(x='# Gears', y=\"CombMPG\", data=df)\n</pre> sns.boxplot(x='# Gears', y=\"CombMPG\", data=df) Out[66]: <pre>&lt;AxesSubplot:xlabel='# Gears', ylabel='CombMPG'&gt;</pre> <p>The box plot shows us that the range of MPG values we see on each gearbox type aren't as crazily distributed as it seemed at first glance; many of the extreme values are in fact outliers that are best discarded when analyzing the trends. This makes the real relationships easier to see; those continuously variable transmissions with a single gear are really quite good at fuel efficiency (higher MPG's are more efficient). Between 5 and 8 things are roughly the same, but from 8-10 things decline markedly.</p> In\u00a0[67]: Copied! <pre>sns.swarmplot(x='# Gears', y=\"CombMPG\", data=df)\n</pre> sns.swarmplot(x='# Gears', y=\"CombMPG\", data=df) <pre>c:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\seaborn\\categorical.py:1296: UserWarning: 14.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.\n  warnings.warn(msg, UserWarning)\n</pre> Out[67]: <pre>&lt;AxesSubplot:xlabel='# Gears', ylabel='CombMPG'&gt;</pre> <p>The swarm plot makes it even more apparent that those high MPG outliers are in fact outliers on the 6-gear vehicles; the vast majority of 6-gear vehcles have less than 32 MPG. And the overall trend is perhaps easiest to visualize and understand in this representation.</p> <p>So, our final verdict: more gears result in worse fuel efficiency. Which strikes me as a little counter-intuitive! But, this is real data from the US Department of Energy for 2019 model year cars, and that's a real conclusion we've drawn, thanks to Seaborn!</p>"},{"location":"01-data-preprocessing/docs/#data-preprocessing","title":"Data Preprocessing\u00b6","text":"<p>We have all heard the phrase \"garbage in, garbage out\", in the realm of Machine Learning, it translates to Bad Data, Bad Results.</p> <p>Throughout this bootcamp we will delve into Python and play with data. We will explore several valuable libraries such as numpy, pandas, scikit et cetera. Python is widely used in ML (Machine Learning) due to it's simplicity in syntax and extensive open source libraries.</p>"},{"location":"01-data-preprocessing/docs/#introduction-to-python","title":"Introduction To Python\u00b6","text":"<p>So what is Python? A Snake? Yes, but also a programming language that encompasses well known concepts such as conditionals (if-else statements), loops (for, while), arithmethic (+,-,*,/) and more. Throughout this course we will explain all the syntax we use, but still a basic understanding can be of use. Let's start simple...</p>"},{"location":"01-data-preprocessing/docs/#the-basics","title":"The Basics\u00b6","text":""},{"location":"01-data-preprocessing/docs/#printing","title":"Printing\u00b6","text":""},{"location":"01-data-preprocessing/docs/#conditionals","title":"Conditionals\u00b6","text":""},{"location":"01-data-preprocessing/docs/#loops","title":"Loops\u00b6","text":"<p>Loops in python are also simple.</p>"},{"location":"01-data-preprocessing/docs/#datatypes","title":"Datatypes\u00b6","text":"<p>All this is great, but what are the datatypes available in this langauge?</p> <p>Well, the common datatypes are integers, floats, booleans and strings. More complex ones would be Lists, Tuples and Dictionaries. They are all fairly simple, and you can learn more about them by checking the links provided.</p>"},{"location":"01-data-preprocessing/docs/#functions","title":"Functions\u00b6","text":""},{"location":"01-data-preprocessing/docs/#python-libraries","title":"Python Libraries\u00b6","text":"<p>Libraries are collections of modules, and modules are simply python code - functions, classes, constants etc. Below is a simple example of this.</p>"},{"location":"01-data-preprocessing/docs/#pandas","title":"Pandas\u00b6","text":"<p>Pandas Docs. The Dataset we will be using for out first foray into Pandas is the Insurance Targets List.</p>"},{"location":"01-data-preprocessing/docs/#data-indexing","title":"Data Indexing\u00b6","text":"<p>Pandas offers two ways to index data:</p> <ul> <li><code>.loc</code> - Label Based Indexing</li> <li><code>.iloc</code> - Integer Based Indexing</li> </ul>"},{"location":"01-data-preprocessing/docs/#loc","title":"<code>.loc</code>\u00b6","text":"<ul> <li>loc interprets values provided as labels (strings).</li> <li>Its often used for conditional indexing.</li> <li>It includes last index.</li> </ul>"},{"location":"01-data-preprocessing/docs/#iloc","title":"<code>.iloc</code>\u00b6","text":"<ul> <li>In pandas we use a method iloc which stands for integer locate</li> <li>It views all input as integer indices</li> <li>in contrast to loc it doesn't include last index</li> </ul>"},{"location":"01-data-preprocessing/docs/#exploring-data","title":"Exploring Data\u00b6","text":"<p>Data exploration is the first step of data analysis used to explore and visualize data to uncover insights from the start or identify areas or patterns to dig into more.</p>"},{"location":"01-data-preprocessing/docs/#unique","title":"<code>.unique()</code>\u00b6","text":"<p>Returns a list of unique values in a given Series</p>"},{"location":"01-data-preprocessing/docs/#value_counts","title":"<code>.value_counts()</code>\u00b6","text":"<p>Returns a Series of count of each value in a given Series</p>"},{"location":"01-data-preprocessing/docs/#inconsistent-data","title":"Inconsistent Data\u00b6","text":"<p>Checking the same for Item Fat Content:</p>"},{"location":"01-data-preprocessing/docs/#missing-values","title":"Missing Values\u00b6","text":"<p>Missing values in datasets are often represented by <code>NaN</code> or <code>None</code>. It usually requires to cleaned before being fed into a model. But before we get into missing values, here's a small revision:</p> <ol> <li>The mean is the average of a data set.</li> <li>The mode is the most common number in a data set.</li> <li>The median is the middle of the set of numbers.</li> </ol> <p></p>"},{"location":"01-data-preprocessing/docs/#normalization","title":"Normalization\u00b6","text":"<p>Normalization is a data preparation technique that is frequently used in machine learning. The process of transforming the columns in a dataset to the same scale is referred to as normalization. Every dataset does not need to be normalized for machine learning. It is only required when the ranges of characteristics are different. (We will learn more in coming sessions)</p>"},{"location":"01-data-preprocessing/docs/#dealing-with-outliers","title":"Dealing with Outliers\u00b6","text":""},{"location":"01-data-preprocessing/docs/#activity","title":"Activity\u00b6","text":""},{"location":"01-data-preprocessing/docs/#matplotlib-basics","title":"MatPlotLib Basics\u00b6","text":""},{"location":"01-data-preprocessing/docs/#draw-a-line-graph","title":"Draw a line graph\u00b6","text":""},{"location":"01-data-preprocessing/docs/#mutiple-plots-on-one-graph","title":"Mutiple Plots on One Graph\u00b6","text":""},{"location":"01-data-preprocessing/docs/#save-it-to-a-file","title":"Save it to a File\u00b6","text":""},{"location":"01-data-preprocessing/docs/#adjust-the-axes","title":"Adjust the Axes\u00b6","text":""},{"location":"01-data-preprocessing/docs/#add-a-grid","title":"Add a Grid\u00b6","text":""},{"location":"01-data-preprocessing/docs/#change-line-types-and-colors","title":"Change Line Types and Colors\u00b6","text":""},{"location":"01-data-preprocessing/docs/#labeling-axes-and-adding-a-legend","title":"Labeling Axes and Adding a Legend\u00b6","text":""},{"location":"01-data-preprocessing/docs/#xkcd-style","title":"XKCD Style :)\u00b6","text":""},{"location":"01-data-preprocessing/docs/#pie-chart","title":"Pie Chart\u00b6","text":""},{"location":"01-data-preprocessing/docs/#bar-chart","title":"Bar Chart\u00b6","text":""},{"location":"01-data-preprocessing/docs/#scatter-plot","title":"Scatter Plot\u00b6","text":""},{"location":"01-data-preprocessing/docs/#histogram","title":"Histogram\u00b6","text":""},{"location":"01-data-preprocessing/docs/#box-whisker-plot","title":"Box &amp; Whisker Plot\u00b6","text":""},{"location":"01-data-preprocessing/docs/#activity","title":"Activity\u00b6","text":""},{"location":"01-data-preprocessing/docs/#seaborn","title":"Seaborn\u00b6","text":"<p>Seaborn is a visualization library that sits on top of matplotlib, making it nicer to look at and adding some extra capabilities too.</p> <p>Let's start by loading up a real dataset on 2019 model-year vehicles, and plotting a histogram just using matplotlib to see the distribution by number of gears.</p>"},{"location":"01-data-preprocessing/docs/#exercise","title":"Exercise\u00b6","text":"<p>Explore the relationship between the number of gears a car has, and its combined MPG rating. Visualize these two dimensions using a scatter plot, lmplot, jointplot, boxplot, and swarmplot. What conclusions can you draw?</p>"},{"location":"02-model-building-i/docs/","title":"Model Building - I","text":"<p>Model building is the process of creating a machine learning model that can predict outcomes based on input data.</p> <p>Once you have preprocessed your data, the next step is to choose an appropriate machine learning algorithm.</p> <p>What is an ML model?</p> <p>In a nutshell,</p> <ul> <li>It's a program that can find patterns or make decisions from a previously unseen dataset.</li> <li>You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from the data.</li> <li>Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data.</li> </ul> <p>Why do we even have types?</p> <p>At it's core, Machine Learning models are just math. Most times, unfortunately, complex maths..</p> <p></p> <p>Machine Learning sits at the intersection of statistics and computer science, yet it can still wear many different masks.</p> <p>When we say Types of machine learning, we actually refer to the type of algorithm used for machine learning.</p> <p>So, for various applications, we have different machine learning algorithms.  Some algorithms give more accurate results over others. </p> <p>There are four types of machine learning:</p> <ol> <li>Supervised Learning</li> <li>Unsupervised Learning</li> <li>Semi-supervised Learning (class of supervised learning)</li> <li>Reinforcement Learning</li> </ol> <p></p> <p>Here is a brief description:</p> <p>Involves machine learning algorithms that learn under the presence of a supervisor.</p> <p></p> <ul> <li>Relies on labelled input and output training data.</li> <li>The example-label pairs are fed one by one, allowing the algorithm to predict the label for each example, giving it feedback as to whether it predicted the right answer or not.</li> <li>Over time, the algorithm will learn to approximate the nature of the relationship between examples and their labels.</li> <li>Training process continues until the model achieves a desired level of accuracy on the training data.</li> </ul> <p>Here is a simple analogy:</p> <p>Consider a programming instructor teaching us students how to write code in a new programming language.</p> <p>The instructor starts by teaching us the basic syntax and programming concepts, and then have us practice writing code to solve simple problems.</p> <p>As we practice, the instructor corrects our mistakes and provides feedback on our code structure, design, variable names, program flow, etc.</p> <p>And over time, we can become more proficient in the language and can start to write more complex programs and solve more challenging problems. The instructor continues to provide feedback and guidance, helping the us improve our programming skills.</p> <p>ML Algorithms:</p> <ul> <li>Regression</li> <li>Decision Tree</li> <li>Random Forest</li> <li>KNN</li> <li>Logistic Regression</li> </ul> <p>Advantages:</p> <ul> <li>The model learns from past experiences, i.e., the introduced data.</li> <li>Availability of a significantly larger pool of algorithms compared to others.</li> </ul> <p>Disadvantages:</p> <ul> <li>Challenging and time-consuming to label massive data</li> <li>Difficult to predict accurately if the distribution of the test data differs significantly from that of the training dataset.</li> </ul> <p>Applications:</p> <ul> <li>Speech recognition</li> <li>Image classification</li> <li>Spam detection</li> <li>Weather Forecast</li> <li>Face Recognition</li> </ul> <p>Throughout this session, we shall focus on ML models based on supervised learning.</p> <p>Unsupervised learning is very much the opposite of supervised learning. It features no labels. Instead, our algorithm would be fed a lot of data and given the tools to understand the properties of the data. From there, it can learn to group, cluster, and/or organize the data in a way such that a human (or other intelligent algorithm) can come in and make sense of the newly organized data.</p> <p>Unsupervised learning is particularly useful in finding unknown patterns in a dataset. It aids in finding features needed for categorization. Your images, videos, or any data provided doesn\u2019t have to be annotated or labeled.</p> <p></p> <p>Involves learning from unlabeled data without any supervision.</p> <ul> <li>No predefined labels or output for the data, and the algorithm must identify patterns or relationships on its own.</li> <li>Algorithm is trained on the input data and tasked with finding hidden structures or relationships within it.</li> <li>The algorithm continues to learn and refine its understanding of the data until it can identify meaningful patterns and structures.</li> </ul> <p>Here is a simple analogy: </p> <p>Think about when you first arrived at college, and wanted to make friends with other students who share similar interests, but you didn't know anyone. To find other like-minded students, you attend different events and gatherings on campus, such as club fairs, sports games, and academic talks.</p> <p>As you interact with other students and attend different events, you start to notice that certain groups of students tend to congregate together and share common interests.</p> <p>For example, you might notice that there is a group of students who are passionate about technology and often participate in hackathons and coding competitions.</p> <p>As you continue to attend different events and interact with more students, you start to identify clusters of students based on their interests and activities.</p> <p>ML Algorithms:</p> <ul> <li>Clustering</li> <li>Principal Component Analysis (PCA)</li> <li>Singular Value Decomposition (SVD)</li> <li>Independent Component Analysis (ICA)</li> <li>K-Means Clustering</li> </ul> <p>Advantages:</p> <ul> <li>Unsupervised learning does not require labeled data, which can be difficult and time-consuming to obtain.</li> <li>This type of learning can uncover new insights and patterns that may not have been noticed by humans.</li> </ul> <p>Disadvantages:</p> <ul> <li>Evaluating the accuracy of an unsupervised learning algorithm is often more difficult than with supervised learning, as there is no clear way to measure correctness.</li> <li>Unsupervised learning can be computationally expensive and may require more complex algorithms and hardware to achieve good results.</li> </ul> <p>Applications:</p> <ul> <li>Anomaly detection</li> <li>Data compression</li> <li>Image segmentation</li> <li>Clustering customers based on behavior or preferences</li> <li>Recommender systems</li> </ul> <p>Involves machine learning algorithms that learn from a combination of labelled and unlabelled data.</p> <p></p> <ul> <li>Typically, a small set of labelled data is provided, along with a much larger set of unlabelled data.</li> <li>Algorithm uses the labelled data to learn patterns in the data and then applies this learning to the unlabelled data.</li> <li>The goal is to use the unlabelled data to improve the accuracy of the model beyond what could be achieved with just the labelled data.</li> </ul> <p>Here is a simple analogy:  Think of it like a chef creating a new recipe with a mix of familiar and unfamiliar ingredients. By experimenting with the known ingredients, the chef identifies patterns and builds a foundation for the recipe. Then, by introducing the unfamiliar ingredients and using the patterns identified with the known ingredients as a guide, the chef is able to create a more complex and interesting recipe than they could have with just the familiar ingredients alone.</p> <p>ML Algorithms:</p> <ul> <li>Self-Training</li> <li>Co-Training</li> <li>Multi-View Learning</li> <li>Semi-Supervised SVM</li> <li>Graph-Based Learning</li> </ul> <p>Advantages:</p> <ul> <li>Less time-consuming and costly than fully-supervised learning, as it requires less labelled data.</li> <li>Can improve the accuracy of a model by incorporating additional unlabelled data.</li> </ul> <p>Disadvantages:</p> <ul> <li>May not be as accurate as fully-supervised learning, since it relies on unlabelled data to provide additional information.</li> <li>Difficult to balance the amount of labelled and unlabelled data for optimal performance.</li> </ul> <p>Applications:</p> <ul> <li>Text classification</li> <li>Image classification</li> <li>Anomaly detection</li> <li>Protein sequence analysis</li> <li>Speech recognition</li> </ul> <p>Involves machine learning algorithms that learn through interactions with an environment.</p> <p></p> <ul> <li>The algorithm learns by taking actions in an environment and receiving feedback in the form of rewards or penalties.</li> <li>The goal is to learn a policy, or a set of rules for selecting actions, that maximizes the cumulative reward over time.</li> </ul> <p>Here is another analogy:</p> <p>Think about the time you were deciding which college extracurricular activities to participate in. You don't know which activities will be the most enjoyable or rewarding, so you try out different options and observe how they affect your mood and overall experience.</p> <p>For example, you might try out for the sports team, music club, or have joined the debate club. After participating in each activity for a while, you evaluate how much you enjoyed it and how much it contributed to your personal growth.</p> <p>Over time, you start to learn which activities are the most enjoyable and beneficial for you. You begin to focus on these activities more and prioritize them over less enjoyable or less rewarding activities.</p> <p>ML Algorithms:</p> <ul> <li>Q-Learning</li> <li>SARSA</li> <li>Deep Q-Networks (DQN)</li> <li>Policy Gradients</li> <li>Actor-Critic</li> </ul> <p>Advantages:</p> <ul> <li>Can learn optimal policies in complex environments where the optimal solution is not known or is too difficult to calculate.</li> <li>Can learn from experience without the need for labelled data.</li> </ul> <p>Disadvantages:</p> <ul> <li>Can be time-consuming and computationally expensive to train, especially for complex environments.</li> <li>Can be difficult to design a reward function that accurately reflects the desired behavior.</li> </ul> <p>Applications:</p> <ul> <li>Game playing</li> <li>Robotics</li> <li>Autonomous driving</li> <li>Recommendation systems</li> <li>Control systems for power grids, water treatment plants, etc.</li> </ul> <p>Throughout this session, we shall focus on ML models based on supervised learning.</p> <p></p> <p>Does this look familiar?</p> <p></p> <p>You all have already done this for your physics lab!</p> <p>The trendline option in excel generates a regression based on the data provided.</p> <p>We will be doing a similar thing in python, but in a more advanced level.</p> <p>A short explanation:</p> <p>Linear regression is a statistical method used to model the relationship between a dependent variable and one or more independent variables. The goal of linear regression is to find the linear equation that best describes the relationship between the variables.</p> <p>In a simple linear regression, there is one independent variable and one dependent variable.</p> <p>The linear equation takes the form of:</p> <p>$y = a + bx$</p> <p>where y is the dependent variable, x is the independent variable, a is the intercept, and b is the slope. The intercept represents the value of y when x is equal to zero, and the slope represents the change in y for a one-unit increase in x.</p> <p>All the math involved in building the model is abstracted away into functions, so that machine learning engineers need not implement the algorithm from scratch!</p> <p>Import necessary libraries</p> In\u00a0[39]: Copied! <pre>import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n</pre> import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt <p>The seaborn library comes with a few data sets built-in, let's check it out!</p> In\u00a0[40]: Copied! <pre>sns.get_dataset_names()\n</pre> sns.get_dataset_names()  Out[40]: <pre>['anagrams',\n 'anscombe',\n 'attention',\n 'brain_networks',\n 'car_crashes',\n 'diamonds',\n 'dots',\n 'dowjones',\n 'exercise',\n 'flights',\n 'fmri',\n 'geyser',\n 'glue',\n 'healthexp',\n 'iris',\n 'mpg',\n 'penguins',\n 'planets',\n 'seaice',\n 'taxis',\n 'tips',\n 'titanic']</pre> <p>Let's load the tips dataset</p> In\u00a0[41]: Copied! <pre>tips_data = sns.load_dataset('tips')\n</pre> tips_data = sns.load_dataset('tips') In\u00a0[42]: Copied! <pre>tips_data\n</pre> tips_data Out[42]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>244 rows \u00d7 7 columns</p> In\u00a0[43]: Copied! <pre>tips_data\n</pre> tips_data Out[43]: total_bill tip sex smoker day time size 0 16.99 1.01 Female No Sun Dinner 2 1 10.34 1.66 Male No Sun Dinner 3 2 21.01 3.50 Male No Sun Dinner 3 3 23.68 3.31 Male No Sun Dinner 2 4 24.59 3.61 Female No Sun Dinner 4 ... ... ... ... ... ... ... ... 239 29.03 5.92 Male No Sat Dinner 3 240 27.18 2.00 Female Yes Sat Dinner 2 241 22.67 2.00 Male Yes Sat Dinner 2 242 17.82 1.75 Male No Sat Dinner 2 243 18.78 3.00 Female No Thur Dinner 2 <p>244 rows \u00d7 7 columns</p> <p>The Tips dataset is a data frame with 244 rows and 7 variables which represents some tipping data where one waiter recorded information about each tip he received over a period of a few months working in one restaurant.</p> <p>The waiter collected several variables:</p> <ul> <li>Tip in dollars, the bill in dollars</li> <li>Sex of the bill payer</li> <li>Whether there were smokers in the party</li> <li>Day of the week</li> <li>Time of day</li> <li>Size of the party.</li> </ul> <p>Exploring the data:</p> In\u00a0[44]: Copied! <pre>sns.pairplot(tips_data)\n</pre> sns.pairplot(tips_data) Out[44]: <pre>&lt;seaborn.axisgrid.PairGrid at 0x24f00869e80&gt;</pre> <p>From the pairplot we can see that there is almost a linear correspondence between total_bill and tip.</p> <p>Let's try to make a linear regression model between <code>total_bill</code> and <code>tip</code>, where the model predicts the tip given to a waiter based on the total bill the customer pays.</p> In\u00a0[45]: Copied! <pre>sns.scatterplot(x='total_bill', y='tip', data=tips_data)\n</pre> sns.scatterplot(x='total_bill', y='tip', data=tips_data) Out[45]: <pre>&lt;AxesSubplot:xlabel='total_bill', ylabel='tip'&gt;</pre> <p>This will create a scatterplot of the <code>total_bill</code> vs <code>tip</code> variables.</p> <p>Checking for a linear model:</p> In\u00a0[46]: Copied! <pre>sns.lmplot(x='total_bill', y='tip', data=tips_data)\n</pre> sns.lmplot(x='total_bill', y='tip', data=tips_data) Out[46]: <pre>&lt;seaborn.axisgrid.FacetGrid at 0x24f0267b370&gt;</pre> <p>This will add a regression line to the scatterplot, showing the relationship between 'total_bill' and 'tip'.</p> <p>First we will set a variable <code>x</code> equal to the numerical features of the tips data and a variable <code>y</code> equal to the <code>tips</code> column.</p> In\u00a0[47]: Copied! <pre>y = tips_data['tip']\n</pre> y = tips_data['tip'] In\u00a0[48]: Copied! <pre>x = tips_data['total_bill']\n</pre> x = tips_data['total_bill'] <p>Use model_selection.train_test_split from sklearn to split the data into training and testing sets.</p> In\u00a0[49]: Copied! <pre>TEST_SIZE = 0.2\n</pre> TEST_SIZE = 0.2 In\u00a0[50]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[51]: Copied! <pre>x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=1)\n</pre> x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=1) In\u00a0[52]: Copied! <pre>from sklearn.linear_model import LinearRegression\n</pre> from sklearn.linear_model import LinearRegression <p>Creating an instance of a linear regression model:</p> In\u00a0[53]: Copied! <pre>lm = LinearRegression(fit_intercept=False)\n</pre> lm = LinearRegression(fit_intercept=False) <p>fit_intercept sets the intercept value to zero.</p> In\u00a0[54]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) <pre>\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n~\\AppData\\Local\\Temp/ipykernel_10436/141332336.py in &lt;module&gt;\n----&gt; 1 lm.fit(x_train, y_train)\n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\linear_model\\_base.py in fit(self, X, y, sample_weight)\n    516         accept_sparse = False if self.positive else ['csr', 'csc', 'coo']\n    517 \n--&gt; 518         X, y = self._validate_data(X, y, accept_sparse=accept_sparse,\n    519                                    y_numeric=True, multi_output=True)\n    520 \n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py in _validate_data(self, X, y, reset, validate_separately, **check_params)\n    431                 y = check_array(y, **check_y_params)\n    432             else:\n--&gt; 433                 X, y = check_X_y(X, y, **check_params)\n    434             out = X, y\n    435 \n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\n     61             extra_args = len(args) - len(all_args)\n     62             if extra_args &lt;= 0:\n---&gt; 63                 return f(*args, **kwargs)\n     64 \n     65             # extra_args &gt; 0\n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py in check_X_y(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\n    869         raise ValueError(\"y cannot be None\")\n    870 \n--&gt; 871     X = check_array(X, accept_sparse=accept_sparse,\n    872                     accept_large_sparse=accept_large_sparse,\n    873                     dtype=dtype, order=order, copy=copy,\n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py in inner_f(*args, **kwargs)\n     61             extra_args = len(args) - len(all_args)\n     62             if extra_args &lt;= 0:\n---&gt; 63                 return f(*args, **kwargs)\n     64 \n     65             # extra_args &gt; 0\n\nc:\\users\\anura\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\utils\\validation.py in check_array(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\n    692             # If input is 1D raise error\n    693             if array.ndim == 1:\n--&gt; 694                 raise ValueError(\n    695                     \"Expected 2D array, got 1D array instead:\\narray={}.\\n\"\n    696                     \"Reshape your data either using array.reshape(-1, 1) if \"\n\nValueError: Expected 2D array, got 1D array instead:\narray=[16.99 19.77 31.71 14.   16.27  7.56 40.55 20.69 21.7  10.33  9.78 32.83\n 12.6  10.63 22.76 32.4  16.58 16.04  7.25 17.51 13.39 12.46 31.85 17.89\n 40.17 48.27 44.3  11.87 15.98 24.71 14.83 13.42 20.29 38.01 12.76 19.49\n 25.71 15.98  9.94 19.08 16.82 34.63 11.69 13.94 16.29 25.29 45.35 18.71\n 28.55 43.11 16.45 12.02 28.17 38.07 10.34 25.56 10.77 21.01 13.28 14.52\n 18.24  8.35 12.03 10.07 15.95 15.36 11.61 22.75 17.26 15.42 13.81 16.\n  8.58 10.59 19.81 13.51 24.01 16.66 12.48 21.5  12.66 18.43 28.15 20.76\n 18.29 16.49 22.42 16.31 13.   20.45 15.69 15.53 10.51 14.07 25.89 23.1\n 21.01 30.46  8.77 16.47 17.82 27.2  23.95 20.08 13.13 22.23 14.73  5.75\n 34.65 18.28 27.05 15.81 10.27 16.21 15.06 10.33 32.68 15.48 24.52 29.85\n 11.35 29.8  39.42  8.52 14.78 24.27 20.92 24.55 18.15  8.51  7.25 22.12\n 17.59 21.58 17.46 12.74 14.31 19.44 34.81 13.37 17.92  9.68 19.82 23.68\n 38.73 18.04  7.51 20.27 15.69  9.55 13.42 32.9  17.31 13.27 15.04 20.29\n 11.38 10.34 26.41 15.77 13.81 18.29 26.88 29.03 34.3  13.03 27.28 11.59\n 12.9  20.23 12.54 41.19 25.   48.17 13.16 48.33 18.64 15.38 17.92  9.6\n 17.81 18.26 17.07 20.53 22.82 16.43 28.44 17.29 12.26 16.4  14.15 26.86\n 17.47 10.07 16.93].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</pre> <p>Hmm, why is the above statement giving an error..</p> <p>Because in most cases linear regression in multivariate, i.e, <code>y</code> depends on more than one variable.</p> <p>Our <code>x</code> is a single dimensional array, but the model only works on multi-dimesional arrays..</p> <p>In our case, we could consider the <code>size</code> column of the data set to take into account the dependence in size as well.</p> <p>random_state sets the seed for the random generator so that we can ensure that the results that we get can be reproduced.</p> <p>test_size sets aside part of the data for testing purpose.</p> In\u00a0[55]: Copied! <pre>TEST_SIZE = 0.1\nRANDOM_STATE = 2\n</pre> TEST_SIZE = 0.1 RANDOM_STATE = 2 In\u00a0[56]: Copied! <pre>x = tips_data[['total_bill', 'size']]\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n</pre> x = tips_data[['total_bill', 'size']] x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE) In\u00a0[57]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) Out[57]: <pre>LinearRegression(fit_intercept=False)</pre> In\u00a0[58]: Copied! <pre>print('Coefficients: \\n', lm.coef_)\n</pre> print('Coefficients: \\n', lm.coef_) <pre>Coefficients: \n [0.09632358 0.37962516]\n</pre> <p>But if you see the pair plot from the initial steps, there is no correlation between size and waiter tip..</p> <p>So, you could reshape the original total_bill array, as suggested by the error message.</p> <p>Reshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.</p> In\u00a0[61]: Copied! <pre>x = tips_data['total_bill'].values.reshape(-1, 1)\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE)\n</pre> x = tips_data['total_bill'].values.reshape(-1, 1) x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=TEST_SIZE, random_state=RANDOM_STATE) In\u00a0[62]: Copied! <pre>lm.fit(x_train, y_train)\n</pre> lm.fit(x_train, y_train) Out[62]: <pre>LinearRegression(fit_intercept=False)</pre> In\u00a0[63]: Copied! <pre>print('Coefficients: \\n', lm.coef_)\n</pre> print('Coefficients: \\n', lm.coef_) <pre>Coefficients: \n [0.14113406]\n</pre> In\u00a0[64]: Copied! <pre>tip_predictions = lm.predict(x_test)\n</pre> tip_predictions = lm.predict(x_test) In\u00a0[65]: Copied! <pre>tip_predictions\n</pre> tip_predictions Out[65]: <pre>array([4.91569945, 3.60738668, 1.20246223, 2.30330792, 3.38016083,\n       4.8408984 , 1.4028726 , 2.83397201, 3.5283516 , 6.82100931,\n       1.44944684, 2.48254819, 2.98639679, 2.40915847, 1.34783031,\n       3.34205464, 1.94906142, 2.25532234, 1.6357438 , 2.79586581,\n       2.29625122, 2.1706419 , 3.39568558, 2.79727715, 2.43597394])</pre> <p>An these are the real values:</p> In\u00a0[66]: Copied! <pre>y_test\n</pre> y_test Out[66]: <pre>85     5.17\n54     4.34\n126    1.48\n93     4.30\n113    2.55\n141    6.70\n53     1.56\n65     3.15\n157    3.75\n212    9.00\n10     1.71\n64     2.64\n89     3.00\n71     3.00\n30     1.45\n3      3.31\n163    2.00\n84     2.03\n217    1.50\n191    4.19\n225    2.50\n101    3.00\n35     3.60\n24     3.18\n152    2.74\nName: tip, dtype: float64</pre> <p>Let's now create a scatterplot of the real test values versus the predicted values.</p> In\u00a0[67]: Copied! <pre>plt.scatter(y_test, tip_predictions)\nplt.xlabel('Y Test')\nplt.ylabel('Predicted Y')\n</pre> plt.scatter(y_test, tip_predictions) plt.xlabel('Y Test') plt.ylabel('Predicted Y') Out[67]: <pre>Text(0, 0.5, 'Predicted Y')</pre> In\u00a0[68]: Copied! <pre>from sklearn.metrics import mean_squared_error, r2_score\n</pre> from sklearn.metrics import mean_squared_error, r2_score In\u00a0[69]: Copied! <pre>mse = mean_squared_error(y_test, tip_predictions)\nr2 = r2_score(y_test, tip_predictions)\n</pre> mse = mean_squared_error(y_test, tip_predictions) r2 = r2_score(y_test, tip_predictions) In\u00a0[70]: Copied! <pre>print('Mean squared error:', mse)\nprint('R-squared:', r2)\n</pre> print('Mean squared error:', mse) print('R-squared:', r2) <pre>Mean squared error: 0.688445648107554\nR-squared: 0.7602517788014659\n</pre> <p>Lower the MSE, the more accurate a model is. An MSE of zero is a perfect model.</p> <p>R-squared gives a accuracy measure in terms of percentage.</p> In\u00a0[71]: Copied! <pre>accuracy = round(r2*100, 2)\nprint(f'Model accuracy \u2248', accuracy, '%')\n</pre> accuracy = round(r2*100, 2) print(f'Model accuracy \u2248', accuracy, '%') <pre>Model accuracy \u2248 76.03 %\n</pre> <p>Let's compare the predicted and actual values:</p> In\u00a0[72]: Copied! <pre>tip_predictions = [round(prediction, 2) for prediction in tip_predictions] #\ny_test = list(y_test)\n</pre> tip_predictions = [round(prediction, 2) for prediction in tip_predictions] # y_test = list(y_test) In\u00a0[73]: Copied! <pre>comparision_df = pd.DataFrame({'Predicted': tip_predictions, 'Actual': y_test})\n</pre> comparision_df = pd.DataFrame({'Predicted': tip_predictions, 'Actual': y_test}) In\u00a0[74]: Copied! <pre>comparision_df\n</pre> comparision_df Out[74]: Predicted Actual 0 4.92 5.17 1 3.61 4.34 2 1.20 1.48 3 2.30 4.30 4 3.38 2.55 5 4.84 6.70 6 1.40 1.56 7 2.83 3.15 8 3.53 3.75 9 6.82 9.00 10 1.45 1.71 11 2.48 2.64 12 2.99 3.00 13 2.41 3.00 14 1.35 1.45 15 3.34 3.31 16 1.95 2.00 17 2.26 2.03 18 1.64 1.50 19 2.80 4.19 20 2.30 2.50 21 2.17 3.00 22 3.40 3.60 23 2.80 3.18 24 2.44 2.74 <p>Linear regression is a powerful method for modeling the relationship between input features and target variables, and Seaborn makes it easy to perform and visualize linear regression in Python.</p> In\u00a0[75]: Copied! <pre>%matplotlib inline\n</pre> %matplotlib inline In\u00a0[76]: Copied! <pre>import io\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport scipy.stats\nimport scipy.special\nimport seaborn as sns\nsns.set_style('white')\nsns.set_context('notebook')\n</pre> import io import matplotlib.pyplot as plt import pandas as pd import numpy as np import scipy.stats import scipy.special import seaborn as sns sns.set_style('white') sns.set_context('notebook') <p>Spider data from Suzuki et al. (2006) In following cells what we analyze is the same as the analysis made in the paper. However we won't go in that direction because of obvious reasons.</p> In\u00a0[77]: Copied! <pre>data = \"\"\"Grain size (mm)\tSpiders\n0.245\tabsent\n0.247\tabsent\n0.285\tpresent\n0.299\tpresent\n0.327\tpresent\n0.347\tpresent\n0.356\tabsent\n0.36\tpresent\n0.363\tabsent\n0.364\tpresent\n0.398\tabsent\n0.4\tpresent\n0.409\tabsent\n0.421\tpresent\n0.432\tabsent\n0.473\tpresent\n0.509\tpresent\n0.529\tpresent\n0.561\tabsent\n0.569\tabsent\n0.594\tpresent\n0.638\tpresent\n0.656\tpresent\n0.816\tpresent\n0.853\tpresent\n0.938\tpresent\n1.036\tpresent\n1.045\tpresent\n\"\"\"\ndf = pd.read_table(io.StringIO(data))\ndf.Spiders = df.Spiders == 'present'\ndf.head()\n</pre> data = \"\"\"Grain size (mm)\tSpiders 0.245\tabsent 0.247\tabsent 0.285\tpresent 0.299\tpresent 0.327\tpresent 0.347\tpresent 0.356\tabsent 0.36\tpresent 0.363\tabsent 0.364\tpresent 0.398\tabsent 0.4\tpresent 0.409\tabsent 0.421\tpresent 0.432\tabsent 0.473\tpresent 0.509\tpresent 0.529\tpresent 0.561\tabsent 0.569\tabsent 0.594\tpresent 0.638\tpresent 0.656\tpresent 0.816\tpresent 0.853\tpresent 0.938\tpresent 1.036\tpresent 1.045\tpresent \"\"\" df = pd.read_table(io.StringIO(data)) df.Spiders = df.Spiders == 'present' df.head() Out[77]: Grain size (mm) Spiders 0 0.245 False 1 0.247 False 2 0.285 True 3 0.299 True 4 0.327 True In\u00a0[78]: Copied! <pre>df\n</pre> df Out[78]: Grain size (mm) Spiders 0 0.245 False 1 0.247 False 2 0.285 True 3 0.299 True 4 0.327 True 5 0.347 True 6 0.356 False 7 0.360 True 8 0.363 False 9 0.364 True 10 0.398 False 11 0.400 True 12 0.409 False 13 0.421 True 14 0.432 False 15 0.473 True 16 0.509 True 17 0.529 True 18 0.561 False 19 0.569 False 20 0.594 True 21 0.638 True 22 0.656 True 23 0.816 True 24 0.853 True 25 0.938 True 26 1.036 True 27 1.045 True In\u00a0[79]: Copied! <pre>df[\"Spiders\"]\n</pre> df[\"Spiders\"] Out[79]: <pre>0     False\n1     False\n2      True\n3      True\n4      True\n5      True\n6     False\n7      True\n8     False\n9      True\n10    False\n11     True\n12    False\n13     True\n14    False\n15     True\n16     True\n17     True\n18    False\n19    False\n20     True\n21     True\n22     True\n23     True\n24     True\n25     True\n26     True\n27     True\nName: Spiders, dtype: bool</pre> In\u00a0[80]: Copied! <pre>plt.scatter(df[\"Grain size (mm)\"], df[\"Spiders\"])\nplt.ylabel('Spiders present?')\nsns.despine()\n</pre> plt.scatter(df[\"Grain size (mm)\"], df[\"Spiders\"]) plt.ylabel('Spiders present?') sns.despine() In\u00a0[81]: Copied! <pre>import sklearn.linear_model\n</pre> import sklearn.linear_model <p>scikit-learn has a logisitic regression classifier which uses regularization. To eliminate regularization, we set the regularization parameter <code>C</code> to $10^{12}$.</p> In\u00a0[82]: Copied! <pre># C=1e12 is effectively no regularization - see https://github.com/scikit-learn/scikit-learn/issues/6738\nclf = sklearn.linear_model.LogisticRegression(C=1e12, random_state=0)\nclf.fit(df['Grain size (mm)'].values.reshape(-1, 1), df['Spiders'])\nprint(clf.intercept_, clf.coef_)\n</pre> # C=1e12 is effectively no regularization - see https://github.com/scikit-learn/scikit-learn/issues/6738 clf = sklearn.linear_model.LogisticRegression(C=1e12, random_state=0) clf.fit(df['Grain size (mm)'].values.reshape(-1, 1), df['Spiders']) print(clf.intercept_, clf.coef_) <pre>[-1.64761964] [[5.12153717]]\n</pre> In\u00a0[83]: Copied! <pre>def plot_log_reg(x, y, data, clf, xmin=None, xmax=None, alpha=1, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    else:\n        fig = ax.figure\n    ax.scatter(data[x], data[y], color='black', zorder=20, alpha=alpha)\n    if xmin is None:\n        xmin = x.min()\n    if xmax is None:\n        xmax = x.max()\n    X_test = np.linspace(xmin, xmax, 300)\n\n    loss = scipy.special.expit(X_test * clf.coef_ + clf.intercept_).ravel()\n    ax.plot(X_test, loss, linewidth=3)\n\n    ax.set_xlabel(x)\n    ax.set_ylabel(y)\n    fig.tight_layout()\n    sns.despine()\n    return fig, ax\n</pre> def plot_log_reg(x, y, data, clf, xmin=None, xmax=None, alpha=1, ax=None):     if ax is None:         fig, ax = plt.subplots()     else:         fig = ax.figure     ax.scatter(data[x], data[y], color='black', zorder=20, alpha=alpha)     if xmin is None:         xmin = x.min()     if xmax is None:         xmax = x.max()     X_test = np.linspace(xmin, xmax, 300)      loss = scipy.special.expit(X_test * clf.coef_ + clf.intercept_).ravel()     ax.plot(X_test, loss, linewidth=3)      ax.set_xlabel(x)     ax.set_ylabel(y)     fig.tight_layout()     sns.despine()     return fig, ax In\u00a0[84]: Copied! <pre>plot_log_reg(x='Grain size (mm)', y='Spiders', data=df, clf=clf, xmin=0, xmax=1.5);\n</pre> plot_log_reg(x='Grain size (mm)', y='Spiders', data=df, clf=clf, xmin=0, xmax=1.5); <p>KNN is a simple concept: define some distance metric between the items in your dataset, and find the K closest items. You can then use those items to predict some property of a test item, by having them somehow \"vote\" on it.</p> <p>As an example, let's look at the MovieLens data. We'll try to guess the rating of a movie by looking at the 10 movies that are closest to it in terms of genres and popularity.</p> <p>To start, we'll load up every rating in the data set into a Pandas DataFrame:</p> In\u00a0[85]: Copied! <pre>import pandas as pd\n\nr_cols = ['user_id', 'movie_id', 'rating']\nratings = pd.read_csv('u.data', sep='\\t', names=r_cols, usecols=range(3))\nratings.head()\n</pre> import pandas as pd  r_cols = ['user_id', 'movie_id', 'rating'] ratings = pd.read_csv('u.data', sep='\\t', names=r_cols, usecols=range(3)) ratings.head() Out[85]: user_id movie_id rating 0 0 50 5 1 0 172 5 2 0 133 1 3 196 242 3 4 186 302 3 <p>Now, we'll group everything by movie ID, and compute the total number of ratings (each movie's popularity) and the average rating for every movie:</p> In\u00a0[86]: Copied! <pre>import numpy as np\n\nmovieProperties = ratings.groupby('movie_id').agg({'rating': [np.size, np.mean]})\nmovieProperties.head()\n</pre> import numpy as np  movieProperties = ratings.groupby('movie_id').agg({'rating': [np.size, np.mean]}) movieProperties.head() Out[86]: rating size mean movie_id 1 452 3.878319 2 131 3.206107 3 90 3.033333 4 209 3.550239 5 86 3.302326 <p>The raw number of ratings isn't very useful for computing distances between movies, so we'll create a new DataFrame that contains the normalized number of ratings. So, a value of 0 means nobody rated it, and a value of 1 will mean it's the most popular movie there is.</p> In\u00a0[87]: Copied! <pre>movieNumRatings = pd.DataFrame(movieProperties['rating']['size'])\nmovieNormalizedNumRatings = movieNumRatings.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x)))\nmovieNormalizedNumRatings.head()\n</pre> movieNumRatings = pd.DataFrame(movieProperties['rating']['size']) movieNormalizedNumRatings = movieNumRatings.apply(lambda x: (x - np.min(x)) / (np.max(x) - np.min(x))) movieNormalizedNumRatings.head() Out[87]: size movie_id 1 0.773585 2 0.222985 3 0.152659 4 0.356775 5 0.145798 <p>Now, let's get the genre information from the u.item file. The way this works is there are 19 fields, each corresponding to a specific genre - a value of '0' means it is not in that genre, and '1' means it is in that genre. A movie may have more than one genre associated with it.</p> <p>While we're at it, we'll put together everything into one big Python dictionary called movieDict. Each entry will contain the movie name, list of genre values, the normalized popularity score, and the average rating for each movie:</p> In\u00a0[88]: Copied! <pre>movieDict = {}\nwith open(r'u.item', encoding=\"ISO-8859-1\") as f:\n    temp = ''\n    for line in f:\n        #line.decode(\"ISO-8859-1\")\n        fields = line.rstrip('\\n').split('|')\n        movieID = int(fields[0])\n        name = fields[1]\n        genres = fields[5:25]\n        genres = map(int, genres)\n        movieDict[movieID] = (name, np.array(list(genres)), movieNormalizedNumRatings.loc[movieID].get('size'), movieProperties.loc[movieID].rating.get('mean'))\n</pre> movieDict = {} with open(r'u.item', encoding=\"ISO-8859-1\") as f:     temp = ''     for line in f:         #line.decode(\"ISO-8859-1\")         fields = line.rstrip('\\n').split('|')         movieID = int(fields[0])         name = fields[1]         genres = fields[5:25]         genres = map(int, genres)         movieDict[movieID] = (name, np.array(list(genres)), movieNormalizedNumRatings.loc[movieID].get('size'), movieProperties.loc[movieID].rating.get('mean'))  <p>For example, here's the record we end up with for movie ID 1, \"Toy Story\":</p> In\u00a0[89]: Copied! <pre>print(movieDict[1])\n</pre> print(movieDict[1]) <pre>('Toy Story (1995)', array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 0.7735849056603774, 3.8783185840707963)\n</pre> <p>Now let's define a function that computes the \"distance\" between two movies based on how similar their genres are, and how similar their popularity is. Just to make sure it works, we'll compute the distance between movie ID's 2 and 4:</p> In\u00a0[90]: Copied! <pre>from scipy import spatial\n\ndef ComputeDistance(a, b):\n    genresA = a[1]\n    genresB = b[1]\n    genreDistance = spatial.distance.cosine(genresA, genresB)\n    popularityA = a[2]\n    popularityB = b[2]\n    popularityDistance = abs(popularityA - popularityB)\n    return genreDistance + popularityDistance\n    \nComputeDistance(movieDict[2], movieDict[4])\n</pre> from scipy import spatial  def ComputeDistance(a, b):     genresA = a[1]     genresB = b[1]     genreDistance = spatial.distance.cosine(genresA, genresB)     popularityA = a[2]     popularityB = b[2]     popularityDistance = abs(popularityA - popularityB)     return genreDistance + popularityDistance      ComputeDistance(movieDict[2], movieDict[4])   Out[90]: <pre>0.8004574042309892</pre> <p>Remember the higher the distance, the less similar the movies are. Let's check what movies 2 and 4 actually are - and confirm they're not really all that similar:</p> In\u00a0[91]: Copied! <pre>print(movieDict[2])\nprint(movieDict[4])\n</pre> print(movieDict[2]) print(movieDict[4])  <pre>('GoldenEye (1995)', array([0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0]), 0.22298456260720412, 3.2061068702290076)\n('Get Shorty (1995)', array([0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 0.3567753001715266, 3.550239234449761)\n</pre> <p>Now, we just need a little code to compute the distance between some given test movie (Toy Story, in this example) and all of the movies in our data set. When the sort those by distance, and print out the K nearest neighbors:</p> In\u00a0[92]: Copied! <pre>import operator\n\ndef getNeighbors(movieID, K):\n    distances = []\n    for movie in movieDict:\n        if (movie != movieID):\n            dist = ComputeDistance(movieDict[movieID], movieDict[movie])\n            distances.append((movie, dist))\n    distances.sort(key=operator.itemgetter(1))\n    neighbors = []\n    for x in range(K):\n        neighbors.append(distances[x][0])\n    return neighbors\n\nK = 10\navgRating = 0\nneighbors = getNeighbors(1, K)\nfor neighbor in neighbors:\n    avgRating += movieDict[neighbor][3]\n    print (movieDict[neighbor][0] + \" \" + str(movieDict[neighbor][3]))\n    \navgRating /= K\n</pre> import operator  def getNeighbors(movieID, K):     distances = []     for movie in movieDict:         if (movie != movieID):             dist = ComputeDistance(movieDict[movieID], movieDict[movie])             distances.append((movie, dist))     distances.sort(key=operator.itemgetter(1))     neighbors = []     for x in range(K):         neighbors.append(distances[x][0])     return neighbors  K = 10 avgRating = 0 neighbors = getNeighbors(1, K) for neighbor in neighbors:     avgRating += movieDict[neighbor][3]     print (movieDict[neighbor][0] + \" \" + str(movieDict[neighbor][3]))      avgRating /= K <pre>Liar Liar (1997) 3.156701030927835\nAladdin (1992) 3.8127853881278537\nWilly Wonka and the Chocolate Factory (1971) 3.6319018404907975\nMonty Python and the Holy Grail (1974) 4.0664556962025316\nFull Monty, The (1997) 3.926984126984127\nGeorge of the Jungle (1997) 2.685185185185185\nBeavis and Butt-head Do America (1996) 2.7884615384615383\nBirdcage, The (1996) 3.4436860068259385\nHome Alone (1990) 3.0875912408759123\nAladdin and the King of Thieves (1996) 2.8461538461538463\n</pre> <p>While we were at it, we computed the average rating of the 10 nearest neighbors to Toy Story:</p> In\u00a0[93]: Copied! <pre>avgRating\n</pre> avgRating Out[93]: <pre>3.3445905900235564</pre> In\u00a0[94]: Copied! <pre>movieDict[1]\n</pre> movieDict[1] Out[94]: <pre>('Toy Story (1995)',\n array([0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n 0.7735849056603774,\n 3.8783185840707963)</pre> In\u00a0[95]: Copied! <pre>from numpy import random, array\n\n#Create fake income/age clusters for N people in k clusters\ndef createClusteredData(N, k):\n    random.seed(10)\n    pointsPerCluster = float(N)/k\n    X = []\n    for i in range (k):\n        incomeCentroid = random.uniform(20000.0, 200000.0)\n        ageCentroid = random.uniform(20.0, 70.0)\n        for j in range(int(pointsPerCluster)):\n            X.append([random.normal(incomeCentroid, 10000.0), random.normal(ageCentroid, 2.0)])\n    X = array(X)\n    return X\n</pre> from numpy import random, array  #Create fake income/age clusters for N people in k clusters def createClusteredData(N, k):     random.seed(10)     pointsPerCluster = float(N)/k     X = []     for i in range (k):         incomeCentroid = random.uniform(20000.0, 200000.0)         ageCentroid = random.uniform(20.0, 70.0)         for j in range(int(pointsPerCluster)):             X.append([random.normal(incomeCentroid, 10000.0), random.normal(ageCentroid, 2.0)])     X = array(X)     return X In\u00a0[96]: Copied! <pre>%matplotlib inline\n\nfrom sklearn.cluster import KMeans\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale\nfrom numpy import random, float\n\ndata = createClusteredData(100, 5)\n\nmodel = KMeans(n_clusters=5)\n\n# Note I'm scaling the data to normalize it! Important for good results.\nmodel = model.fit(scale(data))\n\n# We can look at the clusters each data point was assigned to\nprint(model.labels_)\n\n# And we'll visualize it:\nplt.figure(figsize=(8, 6))\nplt.scatter(data[:,0], data[:,1], c=model.labels_.astype(float))\nplt.show()\n</pre> %matplotlib inline  from sklearn.cluster import KMeans import matplotlib.pyplot as plt from sklearn.preprocessing import scale from numpy import random, float  data = createClusteredData(100, 5)  model = KMeans(n_clusters=5)  # Note I'm scaling the data to normalize it! Important for good results. model = model.fit(scale(data))  # We can look at the clusters each data point was assigned to print(model.labels_)  # And we'll visualize it: plt.figure(figsize=(8, 6)) plt.scatter(data[:,0], data[:,1], c=model.labels_.astype(float)) plt.show() <pre>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n 3 3 3 3 3 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n</pre> In\u00a0[97]: Copied! <pre>len(data)\n</pre> len(data) Out[97]: <pre>100</pre> <p>Within cluster sum of square</p> <ul> <li>If zero means all data points are separte clusters, hence not helpful</li> <li>If max means all data points are in same cluster</li> </ul> <p>Hence a middle ground, low WCCS value required</p> In\u00a0[98]: Copied! <pre>model.inertia_\n</pre> model.inertia_ Out[98]: <pre>5.300772956616055</pre> In\u00a0[99]: Copied! <pre>wcss = []\nfor i in range(1,101):\n    model = KMeans(i)\n    model.fit(data)\n    wcss_iter = model.inertia_\n    wcss.append(wcss_iter)\n</pre> wcss = [] for i in range(1,101):     model = KMeans(i)     model.fit(data)     wcss_iter = model.inertia_     wcss.append(wcss_iter) In\u00a0[100]: Copied! <pre>number_clusters = range(1,101)\nplt.plot(number_clusters, wcss)\nplt.title(\"Elbow method\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within cluster sum of square\")\n</pre> number_clusters = range(1,101) plt.plot(number_clusters, wcss) plt.title(\"Elbow method\") plt.xlabel(\"Number of clusters\") plt.ylabel(\"Within cluster sum of square\") Out[100]: <pre>Text(0, 0.5, 'Within cluster sum of square')</pre> In\u00a0[101]: Copied! <pre>number_clusters = range(1,11)\nplt.plot(number_clusters, wcss[0:10])\nplt.title(\"Elbow method\")\nplt.xlabel(\"Number of clusters\")\nplt.ylabel(\"Within cluster sum of square\")\n</pre> number_clusters = range(1,11) plt.plot(number_clusters, wcss[0:10]) plt.title(\"Elbow method\") plt.xlabel(\"Number of clusters\") plt.ylabel(\"Within cluster sum of square\") Out[101]: <pre>Text(0, 0.5, 'Within cluster sum of square')</pre> <p>From the graph we can conclude that 4 should be our choice for number of clusters</p>"},{"location":"02-model-building-i/docs/#model-building-i","title":"Model Building - I\u00b6","text":""},{"location":"02-model-building-i/docs/#types-of-machine-learning","title":"Types of Machine Learning\u00b6","text":""},{"location":"02-model-building-i/docs/#supervised-learning","title":"Supervised Learning\u00b6","text":""},{"location":"02-model-building-i/docs/#unsupervised-learning","title":"Unsupervised Learning\u00b6","text":""},{"location":"02-model-building-i/docs/#semi-supervised-learning","title":"Semi Supervised Learning\u00b6","text":""},{"location":"02-model-building-i/docs/#reinforcement-learning","title":"Reinforcement Learning\u00b6","text":""},{"location":"02-model-building-i/docs/#an-explanation-in-a-nutshell","title":"An explanation in a nutshell:\u00b6","text":""},{"location":"02-model-building-i/docs/#linear-regression","title":"Linear Regression\u00b6","text":""},{"location":"02-model-building-i/docs/#loading-the-data-set","title":"Loading the data set\u00b6","text":""},{"location":"02-model-building-i/docs/#visualizing-the-data","title":"Visualizing the data:\u00b6","text":""},{"location":"02-model-building-i/docs/#training-and-testing-data","title":"Training and Testing Data\u00b6","text":"<p>Now that we've explored the data a bit, let's go ahead and split the data into training and testing sets.</p>"},{"location":"02-model-building-i/docs/#training-the-model","title":"Training the Model\u00b6","text":"<p>Now its time to train our model on our training data!</p>"},{"location":"02-model-building-i/docs/#predicting-test-data","title":"Predicting Test Data\u00b6","text":"<p>Now that we have fit our model, let's evaluate its performance by predicting off the test values!</p>"},{"location":"02-model-building-i/docs/#evaluating-the-model","title":"Evaluating the model:\u00b6","text":"<p>You can use metrics such as the mean squared error (MSE) or the coefficient of determination (R-squared) to evaluate the accuracy of the linear regression model.</p>"},{"location":"02-model-building-i/docs/#logistic-regression","title":"Logistic Regression\u00b6","text":"<p>Its similar to linear regression with a bit of difference. Linear Regression is used to handle regression problems whereas Logistic regression is used to handle the classification problems. Linear regression provides a continuous output but Logistic regression provides discreet output.</p>"},{"location":"02-model-building-i/docs/#knn-k-nearest-neighbors","title":"KNN (K-Nearest-Neighbors)\u00b6","text":""},{"location":"02-model-building-i/docs/#k-means-clustering-and-elbow-method","title":"K Means Clustering and Elbow Method\u00b6","text":""},{"location":"03-model-building-ii/docs/","title":"Model Building - II","text":"In\u00a0[1]: Copied! <pre># The script below is to download the required dataset from github\n\n!git clone https://github.com/acmbpdc/ml-bootcamp-2023.git\n\n\n!cp /content/ml-bootcamp-2023/docs/03-model-building-ii/Job_Placement_Data.csv /content\n</pre> # The script below is to download the required dataset from github  !git clone https://github.com/acmbpdc/ml-bootcamp-2023.git   !cp /content/ml-bootcamp-2023/docs/03-model-building-ii/Job_Placement_Data.csv /content <pre>Cloning into 'ml-bootcamp-2023'...\nremote: Enumerating objects: 208, done.\nremote: Counting objects: 100% (86/86), done.\nremote: Compressing objects: 100% (49/49), done.\nremote: Total 208 (delta 43), reused 56 (delta 36), pack-reused 122\nReceiving objects: 100% (208/208), 10.22 MiB | 14.74 MiB/s, done.\nResolving deltas: 100% (66/66), done.\n</pre> In\u00a0[1]: Copied! <pre>\n</pre> In\u00a0[2]: Copied! <pre>import numpy as np\nimport pandas as pd\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\n\nfrom sklearn import metrics\nfrom sklearn import tree \nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import figure\n</pre> import numpy as np import pandas as pd  from sklearn.tree import DecisionTreeClassifier from sklearn.tree import DecisionTreeRegressor  from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import RandomForestRegressor  from xgboost import XGBRegressor from xgboost import XGBClassifier  from sklearn import metrics from sklearn import tree  import matplotlib.pyplot as plt from matplotlib.pyplot import figure <p>In the above example, we create a decision tree to determine if we will accept a certain job offer.</p> <p>Below we have implemented the above processs in python.</p> <p>To convert the dataset to numbers:</p> <ol> <li>Yes / No becomes 1 and 0.</li> <li>The company type is written like this (corporation - 0, startup - 1, mid-teir - 2, conglomerate - 3).</li> </ol> <p>X is a variable with the inputs and Y is a variable with the targets.</p> <p>To create the modelwe only need to specify a max_depth which is the maximum number of decisions it is allowed to take before it has to give us an answer. Larger max_depth give better accuracy but smaller max_depth models run and train faster. Based on your data try tweaking this to give the best results.</p> In\u00a0[3]: Copied! <pre>X = np.array([[55000, 0, 0, 0],\n              [50000, 1, 1, 1],\n              [45000, 1, 1, 2],\n              [49000, 0, 1, 1],\n              [69000, 0, 0, 3],\n              [75000, 1, 0, 0],\n              [50000, 0, 1, 2],\n              [55000, 1, 1, 3]])\n\nY = np.array([0, 1, 0, 0, 0, 1, 1, 1])\n\nclf_tree = DecisionTreeClassifier(max_depth = 3, random_state=6)\n\nclf_tree.fit(X, Y)\n\nprint(\"Loss :\", metrics.log_loss(clf_tree.predict(X), Y))\n</pre> X = np.array([[55000, 0, 0, 0],               [50000, 1, 1, 1],               [45000, 1, 1, 2],               [49000, 0, 1, 1],               [69000, 0, 0, 3],               [75000, 1, 0, 0],               [50000, 0, 1, 2],               [55000, 1, 1, 3]])  Y = np.array([0, 1, 0, 0, 0, 1, 1, 1])  clf_tree = DecisionTreeClassifier(max_depth = 3, random_state=6)  clf_tree.fit(X, Y)  print(\"Loss :\", metrics.log_loss(clf_tree.predict(X), Y)) <pre>Loss : 2.2204460492503136e-16\n</pre> In\u00a0[4]: Copied! <pre>figure(figsize=(20, 6), dpi=200)\n\ntree.plot_tree(clf_tree, filled = True, feature_names = [\"Salary\", \"Is Near Home\", \"Offers Cab\", \"Company Type\"], class_names = [\"No\", \"Yes\"])\n\n\nplt.show()\n</pre> figure(figsize=(20, 6), dpi=200)  tree.plot_tree(clf_tree, filled = True, feature_names = [\"Salary\", \"Is Near Home\", \"Offers Cab\", \"Company Type\"], class_names = [\"No\", \"Yes\"])   plt.show() In\u00a0[5]: Copied! <pre>data = pd.read_csv(\"/content/Job_Placement_Data.csv\")\n\ndata = data.drop(\"specialisation\", axis = 1)\ndata.head()\n</pre> data = pd.read_csv(\"/content/Job_Placement_Data.csv\")  data = data.drop(\"specialisation\", axis = 1) data.head() Out[5]: gender ssc_percentage ssc_board hsc_percentage hsc_board hsc_subject degree_percentage undergrad_degree work_experience emp_test_percentage mba_percent status 0 M 67.00 Others 91.00 Others Commerce 58.00 Sci&amp;Tech No 55.0 58.80 Placed 1 M 79.33 Central 78.33 Others Science 77.48 Sci&amp;Tech Yes 86.5 66.28 Placed 2 M 65.00 Central 68.00 Central Arts 64.00 Comm&amp;Mgmt No 75.0 57.80 Placed 3 M 56.00 Central 52.00 Central Science 52.00 Sci&amp;Tech No 66.0 59.43 Not Placed 4 M 85.80 Central 73.60 Central Commerce 73.30 Comm&amp;Mgmt No 96.8 55.50 Placed In\u00a0[6]: Copied! <pre># Convert the string columns to numerical datatypes\n\ndata[\"gender\"] = data[\"gender\"].replace([\"M\"], 0)\ndata[\"gender\"] = data[\"gender\"].replace([\"F\"], 1)\ndata[\"gender\"] = data[\"gender\"].astype(int)\n\ndata[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Others\"], 0)\ndata[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Central\"], 1)\ndata[\"ssc_board\"] = data[\"ssc_board\"].astype(int)\n\ndata[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Others\"], 0)\ndata[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Central\"], 1)\ndata[\"hsc_board\"] = data[\"hsc_board\"].astype(int)\n\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Commerce\"], 0)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Science\"], 1)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Arts\"], 2)\ndata[\"hsc_subject\"] = data[\"hsc_subject\"].astype(int)\n\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Sci&amp;Tech\"], 0)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Comm&amp;Mgmt\"], 1)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Others\"], 2)\ndata[\"undergrad_degree\"] = data[\"undergrad_degree\"].astype(int)\n\ndata[\"work_experience\"] = data[\"work_experience\"].replace([\"No\"], 0)\ndata[\"work_experience\"] = data[\"work_experience\"].replace([\"Yes\"], 1)\ndata[\"work_experience\"] = data[\"work_experience\"].astype(int)\n\ndata[\"status\"] = data[\"status\"].replace([\"Not Placed\"], 0)\ndata[\"status\"] = data[\"status\"].replace([\"Placed\"], 1)\ndata[\"status\"] = data[\"status\"].astype(int)\n\ndata.head()\n</pre> # Convert the string columns to numerical datatypes  data[\"gender\"] = data[\"gender\"].replace([\"M\"], 0) data[\"gender\"] = data[\"gender\"].replace([\"F\"], 1) data[\"gender\"] = data[\"gender\"].astype(int)  data[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Others\"], 0) data[\"ssc_board\"] = data[\"ssc_board\"].replace([\"Central\"], 1) data[\"ssc_board\"] = data[\"ssc_board\"].astype(int)  data[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Others\"], 0) data[\"hsc_board\"] = data[\"hsc_board\"].replace([\"Central\"], 1) data[\"hsc_board\"] = data[\"hsc_board\"].astype(int)  data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Commerce\"], 0) data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Science\"], 1) data[\"hsc_subject\"] = data[\"hsc_subject\"].replace([\"Arts\"], 2) data[\"hsc_subject\"] = data[\"hsc_subject\"].astype(int)  data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Sci&amp;Tech\"], 0) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Comm&amp;Mgmt\"], 1) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].replace([\"Others\"], 2) data[\"undergrad_degree\"] = data[\"undergrad_degree\"].astype(int)  data[\"work_experience\"] = data[\"work_experience\"].replace([\"No\"], 0) data[\"work_experience\"] = data[\"work_experience\"].replace([\"Yes\"], 1) data[\"work_experience\"] = data[\"work_experience\"].astype(int)  data[\"status\"] = data[\"status\"].replace([\"Not Placed\"], 0) data[\"status\"] = data[\"status\"].replace([\"Placed\"], 1) data[\"status\"] = data[\"status\"].astype(int)  data.head() Out[6]: gender ssc_percentage ssc_board hsc_percentage hsc_board hsc_subject degree_percentage undergrad_degree work_experience emp_test_percentage mba_percent status 0 0 67.00 0 91.00 0 0 58.00 0 0 55.0 58.80 1 1 0 79.33 1 78.33 0 1 77.48 0 1 86.5 66.28 1 2 0 65.00 1 68.00 1 2 64.00 1 0 75.0 57.80 1 3 0 56.00 1 52.00 1 1 52.00 0 0 66.0 59.43 0 4 0 85.80 1 73.60 1 0 73.30 1 0 96.8 55.50 1 In\u00a0[7]: Copied! <pre># Create training and testing data\n\n\nthresh = int(len(data) * 90 / 100) # 90% of the data for training, 10% for testing\n\nX_train_clf = data.drop(\"status\", axis = 1).to_numpy()[:thresh]\nY_train_clf = data[\"status\"].to_numpy()[:thresh]\n\nX_test_clf = data.drop(\"status\", axis = 1).to_numpy()[thresh:]\nY_test_clf = data[\"status\"].to_numpy()[thresh:]\n</pre> # Create training and testing data   thresh = int(len(data) * 90 / 100) # 90% of the data for training, 10% for testing  X_train_clf = data.drop(\"status\", axis = 1).to_numpy()[:thresh] Y_train_clf = data[\"status\"].to_numpy()[:thresh]  X_test_clf = data.drop(\"status\", axis = 1).to_numpy()[thresh:] Y_test_clf = data[\"status\"].to_numpy()[thresh:] <p>We now train a model on this dataset. We have used various models to demonstrate the effect that a change in max_depth can have.</p> In\u00a0[8]: Copied! <pre>clf_tree_3 = DecisionTreeClassifier(max_depth = 3, random_state=6)\n\nclf_tree_3.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_tree_3.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_tree_3.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_3 = DecisionTreeClassifier(max_depth = 3, random_state=6)  clf_tree_3.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_tree_3.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_tree_3.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 4.668867019315694\nTesting Loss : 4.915043643970521\n</pre> In\u00a0[9]: Copied! <pre>clf_tree_5 = DecisionTreeClassifier(max_depth = 5, random_state=6)\n\nclf_tree_5.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_tree_5.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_tree_5.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_5 = DecisionTreeClassifier(max_depth = 5, random_state=6)  clf_tree_5.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_tree_5.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_tree_5.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.4278108500441604\nTesting Loss : 1.638347881323507\n</pre> In\u00a0[10]: Copied! <pre>clf_tree_10 = DecisionTreeClassifier(max_depth = 10, random_state=6)\n\nclf_tree_10.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_tree_10.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_tree_10.predict(X_test_clf), Y_test_clf))\n</pre> clf_tree_10 = DecisionTreeClassifier(max_depth = 10, random_state=6)  clf_tree_10.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_tree_10.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_tree_10.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.2204460492503136e-16\nTesting Loss : 3.276695762647014\n</pre> In\u00a0[11]: Copied! <pre># Visualisation of the best tree\n\ncolumns_clf = ['gender', 'ssc_percentage', 'ssc_board', 'hsc_percentage', 'hsc_board',\n       'hsc_subject', 'degree_percentage', 'undergrad_degree',\n       'work_experience', 'emp_test_percentage', 'mba_percent']\n\nfigure(figsize=(20, 6), dpi=200)\n\ntree.plot_tree(clf_tree_5, filled = True, feature_names = columns_clf, class_names = [\"Not Placed\", \"Placed\"])\n\n\nplt.show()\n</pre> # Visualisation of the best tree  columns_clf = ['gender', 'ssc_percentage', 'ssc_board', 'hsc_percentage', 'hsc_board',        'hsc_subject', 'degree_percentage', 'undergrad_degree',        'work_experience', 'emp_test_percentage', 'mba_percent']  figure(figsize=(20, 6), dpi=200)  tree.plot_tree(clf_tree_5, filled = True, feature_names = columns_clf, class_names = [\"Not Placed\", \"Placed\"])   plt.show() <p>Sklearn also allows us to see how important certain features are. This returns a percentage where a percentage x at index i means that the feature i had an x% effect on the outcome.</p> In\u00a0[12]: Copied! <pre>imp = clf_tree_3.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_tree_3.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 0.0% to the final outcome.\nThe feature ssc_percentage contributed 66.48353667011999% to the final outcome.\nThe feature ssc_board contributed 0.0% to the final outcome.\nThe feature hsc_percentage contributed 23.97924563855191% to the final outcome.\nThe feature hsc_board contributed 0.0% to the final outcome.\nThe feature hsc_subject contributed 0.0% to the final outcome.\nThe feature degree_percentage contributed 0.0% to the final outcome.\nThe feature undergrad_degree contributed 0.0% to the final outcome.\nThe feature work_experience contributed 0.0% to the final outcome.\nThe feature emp_test_percentage contributed 3.938604110728931% to the final outcome.\nThe feature mba_percent contributed 5.598613580599159% to the final outcome.\n</pre> In\u00a0[13]: Copied! <pre>imp = clf_tree_5.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_tree_5.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 0.0% to the final outcome.\nThe feature ssc_percentage contributed 50.43056953304579% to the final outcome.\nThe feature ssc_board contributed 3.828157639961312% to the final outcome.\nThe feature hsc_percentage contributed 20.05383678208535% to the final outcome.\nThe feature hsc_board contributed 0.0% to the final outcome.\nThe feature hsc_subject contributed 2.5265840423744668% to the final outcome.\nThe feature degree_percentage contributed 11.790404432890995% to the final outcome.\nThe feature undergrad_degree contributed 0.0% to the final outcome.\nThe feature work_experience contributed 1.9651209218468062% to the final outcome.\nThe feature emp_test_percentage contributed 5.045560848413108% to the final outcome.\nThe feature mba_percent contributed 4.3597657993821946% to the final outcome.\n</pre> In\u00a0[14]: Copied! <pre>imp = clf_tree_10.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_tree_10.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 4.793840039741679% to the final outcome.\nThe feature ssc_percentage contributed 42.02240769612596% to the final outcome.\nThe feature ssc_board contributed 0.0% to the final outcome.\nThe feature hsc_percentage contributed 19.502758524823726% to the final outcome.\nThe feature hsc_board contributed 0.0% to the final outcome.\nThe feature hsc_subject contributed 2.0545028741750064% to the final outcome.\nThe feature degree_percentage contributed 11.68573279919383% to the final outcome.\nThe feature undergrad_degree contributed 0.0% to the final outcome.\nThe feature work_experience contributed 0.0% to the final outcome.\nThe feature emp_test_percentage contributed 8.488109277317655% to the final outcome.\nThe feature mba_percent contributed 11.452648788622147% to the final outcome.\n</pre> In\u00a0[15]: Copied! <pre>train_data = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")\n\ntrain_data.head()\n</pre> train_data = pd.read_csv(\"/content/sample_data/california_housing_train.csv\")  train_data.head() Out[15]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 0 -114.31 34.19 15.0 5612.0 1283.0 1015.0 472.0 1.4936 66900.0 1 -114.47 34.40 19.0 7650.0 1901.0 1129.0 463.0 1.8200 80100.0 2 -114.56 33.69 17.0 720.0 174.0 333.0 117.0 1.6509 85700.0 3 -114.57 33.64 14.0 1501.0 337.0 515.0 226.0 3.1917 73400.0 4 -114.57 33.57 20.0 1454.0 326.0 624.0 262.0 1.9250 65500.0 In\u00a0[16]: Copied! <pre>test_data = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")\n\ntest_data.head()\n</pre> test_data = pd.read_csv(\"/content/sample_data/california_housing_test.csv\")  test_data.head() Out[16]: longitude latitude housing_median_age total_rooms total_bedrooms population households median_income median_house_value 0 -122.05 37.37 27.0 3885.0 661.0 1537.0 606.0 6.6085 344700.0 1 -118.30 34.26 43.0 1510.0 310.0 809.0 277.0 3.5990 176500.0 2 -117.81 33.78 27.0 3589.0 507.0 1484.0 495.0 5.7934 270500.0 3 -118.36 33.82 28.0 67.0 15.0 49.0 11.0 6.1359 330000.0 4 -119.67 36.33 19.0 1241.0 244.0 850.0 237.0 2.9375 81700.0 In\u00a0[17]: Copied! <pre>train_data.columns\n</pre> train_data.columns Out[17]: <pre>Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income',\n       'median_house_value'],\n      dtype='object')</pre> In\u00a0[18]: Copied! <pre>X_train_reg = train_data.drop(\"median_house_value\", axis = 1).to_numpy()\nY_train_reg = train_data[\"median_house_value\"].to_numpy()\n\nX_test_reg = test_data.drop(\"median_house_value\", axis = 1).to_numpy()\nY_test_reg = test_data[\"median_house_value\"].to_numpy()\n</pre> X_train_reg = train_data.drop(\"median_house_value\", axis = 1).to_numpy() Y_train_reg = train_data[\"median_house_value\"].to_numpy()  X_test_reg = test_data.drop(\"median_house_value\", axis = 1).to_numpy() Y_test_reg = test_data[\"median_house_value\"].to_numpy() In\u00a0[19]: Copied! <pre>reg_tree = DecisionTreeRegressor(max_depth = 10, random_state=6)\n\nreg_tree.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg))\n</pre> reg_tree = DecisionTreeRegressor(max_depth = 10, random_state=6)  reg_tree.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 30800.111026813956\nTesting Loss : 40980.01788411865\n</pre> In\u00a0[20]: Copied! <pre># Visualisation of the best tree\n\nreg_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n       'total_bedrooms', 'population', 'households', 'median_income']\n\nfigure(figsize=(20, 15), dpi=400)\n\ntree.plot_tree(reg_tree, filled = True, feature_names = reg_columns, \n               max_depth = 3, fontsize=10) # limit the tree to only show the first 3 nodes\n\n\nplt.show()\n</pre> # Visualisation of the best tree  reg_columns = ['longitude', 'latitude', 'housing_median_age', 'total_rooms',        'total_bedrooms', 'population', 'households', 'median_income']  figure(figsize=(20, 15), dpi=400)  tree.plot_tree(reg_tree, filled = True, feature_names = reg_columns,                 max_depth = 3, fontsize=10) # limit the tree to only show the first 3 nodes   plt.show() In\u00a0[21]: Copied! <pre>imp = reg_tree.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = reg_tree.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature longitude contributed 15.563208438870904% to the final outcome.\nThe feature latitude contributed 16.638618610215868% to the final outcome.\nThe feature housing_median_age contributed 5.47142054781178% to the final outcome.\nThe feature total_rooms contributed 0.756339018155957% to the final outcome.\nThe feature total_bedrooms contributed 1.1302896974804952% to the final outcome.\nThe feature population contributed 1.3768003197204541% to the final outcome.\nThe feature households contributed 0.7456319540808211% to the final outcome.\nThe feature median_income contributed 58.31769141366372% to the final outcome.\n</pre> <p>We can plot these feature importantances on a bar graph to visualise the distribution among the features.</p> In\u00a0[22]: Copied! <pre>figure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_tree.feature_importances_)\nplt.title(\"Regression Feature Importantance for Regression Tree\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_tree.feature_importances_) plt.title(\"Regression Feature Importantance for Regression Tree\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <p></p> <p></p> In\u00a0[23]: Copied! <pre>clf_rf = RandomForestClassifier(n_estimators = 10, max_depth=4, n_jobs = -1, random_state=3)\n\nclf_rf.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_rf.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf))\n</pre> clf_rf = RandomForestClassifier(n_estimators = 10, max_depth=4, n_jobs = -1, random_state=3)  clf_rf.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_rf.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.6145655308167886\nTesting Loss : 3.276695762647014\n</pre> <p>We find the overall usefulness of our features. It can be insightful to learn how different types of models utilise the same features.</p> In\u00a0[24]: Copied! <pre>imp = clf_rf.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_rf.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 2.476268002793741% to the final outcome.\nThe feature ssc_percentage contributed 39.17332987490621% to the final outcome.\nThe feature ssc_board contributed 0.7069502237031067% to the final outcome.\nThe feature hsc_percentage contributed 26.09725934630589% to the final outcome.\nThe feature hsc_board contributed 0.5067132545473519% to the final outcome.\nThe feature hsc_subject contributed 0.6717982121899062% to the final outcome.\nThe feature degree_percentage contributed 13.244724987267862% to the final outcome.\nThe feature undergrad_degree contributed 0.4140200215186999% to the final outcome.\nThe feature work_experience contributed 3.6352319689183274% to the final outcome.\nThe feature emp_test_percentage contributed 5.314210141235138% to the final outcome.\nThe feature mba_percent contributed 7.759493966613765% to the final outcome.\n</pre> <p>We can also extract and view the individual trees in our random forests. Where clf_rf.estimators_ returns an array of all the trees.</p> In\u00a0[25]: Copied! <pre>rf_trees = clf_rf.estimators_\n\n\ntree_0 = rf_trees[0]\n\nfigure(figsize=(20, 6), dpi=200)\n\ntree.plot_tree(tree_0, filled = True, feature_names = columns_clf, class_names = [\"Not Placed\", \"Placed\"], max_depth = 3)\n\n\nplt.show()\n</pre> rf_trees = clf_rf.estimators_   tree_0 = rf_trees[0]  figure(figsize=(20, 6), dpi=200)  tree.plot_tree(tree_0, filled = True, feature_names = columns_clf, class_names = [\"Not Placed\", \"Placed\"], max_depth = 3)   plt.show() In\u00a0[26]: Copied! <pre>reg_rf = RandomForestRegressor(n_estimators = 50, max_depth=15, n_jobs = -1, random_state=3)\n\nreg_rf.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg))\n</pre> reg_rf = RandomForestRegressor(n_estimators = 50, max_depth=15, n_jobs = -1, random_state=3)  reg_rf.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 16885.276048778334\nTesting Loss : 32885.32294525358\n</pre> In\u00a0[27]: Copied! <pre>imp = reg_rf.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = reg_rf.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature longitude contributed 16.289192107492408% to the final outcome.\nThe feature latitude contributed 15.194999333209443% to the final outcome.\nThe feature housing_median_age contributed 5.924326019352224% to the final outcome.\nThe feature total_rooms contributed 2.1172321958206175% to the final outcome.\nThe feature total_bedrooms contributed 2.4519246097281338% to the final outcome.\nThe feature population contributed 3.2494812901481893% to the final outcome.\nThe feature households contributed 1.8252991866551616% to the final outcome.\nThe feature median_income contributed 52.94754525759383% to the final outcome.\n</pre> In\u00a0[28]: Copied! <pre>figure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_rf.feature_importances_)\nplt.title(\"Regression Feature Importantance for Random Forest\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_rf.feature_importances_) plt.title(\"Regression Feature Importantance for Random Forest\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() <p></p> <p>Note that the xgboost library stores trees iin a different format and therefore is not compatible with the plot_tree method of sklearn.</p> In\u00a0[29]: Copied! <pre>clf_xgb = XGBClassifier(n_estimators = 20, max_depth=7, n_jobs = -1, seed=3)\n\nclf_xgb.fit(X_train_clf, Y_train_clf)\n\nprint(\"Training Loss :\", metrics.log_loss(clf_xgb.predict(X_train_clf), Y_train_clf))\n\nprint(\"Testing Loss :\", metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf))\n</pre> clf_xgb = XGBClassifier(n_estimators = 20, max_depth=7, n_jobs = -1, seed=3)  clf_xgb.fit(X_train_clf, Y_train_clf)  print(\"Training Loss :\", metrics.log_loss(clf_xgb.predict(X_train_clf), Y_train_clf))  print(\"Testing Loss :\", metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)) <pre>Training Loss : 2.2204460492503136e-16\nTesting Loss : 1.638347881323507\n</pre> In\u00a0[30]: Copied! <pre>imp = clf_xgb.feature_importances_\n\nfor i in range(len(columns_clf)):\n  print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = clf_xgb.feature_importances_  for i in range(len(columns_clf)):   print(f\"The feature {columns_clf[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature gender contributed 6.948573887348175% to the final outcome.\nThe feature ssc_percentage contributed 31.114697456359863% to the final outcome.\nThe feature ssc_board contributed 7.817109674215317% to the final outcome.\nThe feature hsc_percentage contributed 12.224050611257553% to the final outcome.\nThe feature hsc_board contributed 9.053606539964676% to the final outcome.\nThe feature hsc_subject contributed 4.565470293164253% to the final outcome.\nThe feature degree_percentage contributed 8.493304997682571% to the final outcome.\nThe feature undergrad_degree contributed 3.12301442027092% to the final outcome.\nThe feature work_experience contributed 5.7991500943899155% to the final outcome.\nThe feature emp_test_percentage contributed 4.420216009020805% to the final outcome.\nThe feature mba_percent contributed 6.4408086240291595% to the final outcome.\n</pre> In\u00a0[31]: Copied! <pre>figure(figsize=(22, 6), dpi=200)\n\nplt.bar(columns_clf, clf_xgb.feature_importances_)\nplt.title(\"Classification Feature Importantance for XGBoost\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(22, 6), dpi=200)  plt.bar(columns_clf, clf_xgb.feature_importances_) plt.title(\"Classification Feature Importantance for XGBoost\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() In\u00a0[32]: Copied! <pre>reg_xgb = XGBRegressor(n_estimators = 10, max_depth=15, n_jobs = -1, seed=3)\n\nreg_xgb.fit(X_train_reg, Y_train_reg)\n\nprint(\"Training Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_train_reg), Y_train_reg))\n\nprint(\"Testing Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg))\n</pre> reg_xgb = XGBRegressor(n_estimators = 10, max_depth=15, n_jobs = -1, seed=3)  reg_xgb.fit(X_train_reg, Y_train_reg)  print(\"Training Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_train_reg), Y_train_reg))  print(\"Testing Loss :\", metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)) <pre>Training Loss : 11537.345794404871\nTesting Loss : 33966.77883138021\n</pre> In\u00a0[33]: Copied! <pre>imp = reg_xgb.feature_importances_\n\nfor i in range(len(reg_columns)):\n  print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\")\n</pre> imp = reg_xgb.feature_importances_  for i in range(len(reg_columns)):   print(f\"The feature {reg_columns[i]} contributed {imp[i] * 100}% to the final outcome.\") <pre>The feature longitude contributed 8.241652697324753% to the final outcome.\nThe feature latitude contributed 11.12857386469841% to the final outcome.\nThe feature housing_median_age contributed 7.736917585134506% to the final outcome.\nThe feature total_rooms contributed 1.9546587020158768% to the final outcome.\nThe feature total_bedrooms contributed 3.9520476013422012% to the final outcome.\nThe feature population contributed 3.655128926038742% to the final outcome.\nThe feature households contributed 2.4359479546546936% to the final outcome.\nThe feature median_income contributed 60.89507341384888% to the final outcome.\n</pre> In\u00a0[34]: Copied! <pre>figure(figsize=(12, 6), dpi=200)\n\nplt.bar(reg_columns, reg_xgb.feature_importances_)\nplt.title(\"Regression Feature Importantance for XGBoost\")\nplt.xlabel(\"Columns\")\nplt.ylabel(\"Features\")\nplt.show()\n</pre> figure(figsize=(12, 6), dpi=200)  plt.bar(reg_columns, reg_xgb.feature_importances_) plt.title(\"Regression Feature Importantance for XGBoost\") plt.xlabel(\"Columns\") plt.ylabel(\"Features\") plt.show() In\u00a0[35]: Copied! <pre>tree_loss = metrics.log_loss(clf_tree_5.predict(X_test_clf), Y_test_clf)\nrf_loss = metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf)\nxgb_loss = metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)\n\n\nfigure(figsize=(6, 4), dpi=200)\n\nplt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss])\nplt.title(\"Model Loss on Classification Task (lower is better)\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Log Loss\")\nplt.show()\n</pre> tree_loss = metrics.log_loss(clf_tree_5.predict(X_test_clf), Y_test_clf) rf_loss = metrics.log_loss(clf_rf.predict(X_test_clf), Y_test_clf) xgb_loss = metrics.log_loss(clf_xgb.predict(X_test_clf), Y_test_clf)   figure(figsize=(6, 4), dpi=200)  plt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss]) plt.title(\"Model Loss on Classification Task (lower is better)\") plt.xlabel(\"Models\") plt.ylabel(\"Log Loss\") plt.show() In\u00a0[36]: Copied! <pre>tree_loss = metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg)\nrf_loss = metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg)\nxgb_loss = metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)\n\n\nfigure(figsize=(6, 4), dpi=200)\n\nplt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss])\nplt.title(\"Model Loss on Regression Task (lower is better)\")\nplt.xlabel(\"Models\")\nplt.ylabel(\"Mean Absolute Error Loss\")\nplt.show()\n</pre> tree_loss = metrics.mean_absolute_error(reg_tree.predict(X_test_reg), Y_test_reg) rf_loss = metrics.mean_absolute_error(reg_rf.predict(X_test_reg), Y_test_reg) xgb_loss = metrics.mean_absolute_error(reg_xgb.predict(X_test_reg), Y_test_reg)   figure(figsize=(6, 4), dpi=200)  plt.bar([\"Decision Tree\", \"Random Forest\", \"XGBoost\"], [tree_loss, rf_loss, xgb_loss]) plt.title(\"Model Loss on Regression Task (lower is better)\") plt.xlabel(\"Models\") plt.ylabel(\"Mean Absolute Error Loss\") plt.show()"},{"location":"03-model-building-ii/docs/#model-building-ii","title":"Model Building - II\u00b6","text":"<p>Now that you know how to build and evaluate basic ML models, we can try to look at some more complex models.</p>"},{"location":"03-model-building-ii/docs/#1-decision-trees","title":"1. Decision Trees\u00b6","text":""},{"location":"03-model-building-ii/docs/#11-what-is-a-decision-tree","title":"1.1 What is a decision tree?\u00b6","text":"<p>Decision trees are a type of supervised learning model that are known to perform very well on labeled tabular datasets. These models work by taking a series of decisions and reaching a conclusion.</p> <p>Let us look at an example</p> <p></p>"},{"location":"03-model-building-ii/docs/#12-how-do-we-create-these-trees","title":"1.2 How do we create these trees?\u00b6","text":"<p>Let us look at an example dataset and see how we can construct the tree above.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 50000 Yes Yes Startup Yes 45000 Yes Yes Mid-teir No 49000 No Yes Startup No 69000 No No Conglomerate No 75000 Yes No Corporation Yes 50000 No Yes Mid-teir Yes 55000 Yes Yes Conglomerate Yes <p>Now we look at the steps to convert this into a tree. Let the \"Do we accept?\" column be the target column.</p> <ol> <li>Find the column that has the greatest impact on the outcome of the target (under the hood we use a formula such as information-gain which tells us how much one column is affected by another column).</li> <li>Split the dataset based on this column.</li> <li>Repeat steps 1 - 2 till we have reached a good solution or our tree has become big enough.</li> </ol> <p>On the above dataset the process will look like this :</p> <ol> <li>Identify Salary as the most influential column and find that the split is around 50,000$.</li> <li>Split the dataset based on this. This gives us the following splits:</li> </ol> <p>Above or equal to 50000$.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No 75000 Yes No Corporation Yes 50000 No Yes Mid-teir Yes 55000 Yes Yes Conglomerate Yes <p>Below 50,000$.</p> Salary Is near home? Offers cab service? Company Type Do we accept? 45000 Yes Yes Mid-teir No 49000 No Yes Startup No <ol> <li>Note that below 50000$ is all no acceptance and label that branch as 'No'.</li> <li>Now split the next tabel based on the next most influential column which is 'Is near home'. Then we get another split.</li> </ol> <p>Is near home = 'Yes'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 Yes Yes Conglomerate Yes 75000 Yes No Corporation Yes <p>Is near home = 'No'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No 50000 No Yes Mid-teir Yes <ol> <li><p>Note that Is near home = 'Yes' has 100% acceptance and label that branch as 'Yes'.</p> </li> <li><p>We do the final split on the column 'Offers cab service' and this gives us splits that have all yes or all no in both of them.</p> </li> </ol> <p>Offers cab service = 'Yes'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 50000 No Yes Mid-teir Yes <p>Offers cab service = 'No'</p> Salary Is near home? Offers cab service? Company Type Do we accept? 55000 No No Corporation No 69000 No No Conglomerate No <ol> <li>This lets us arrive at a final answer.</li> </ol>"},{"location":"03-model-building-ii/docs/#13-visulization","title":"1.3 Visulization\u00b6","text":"<p>We can use the in-built plot_tree method to draw a graphical representation of the tree in sklearn.</p>"},{"location":"03-model-building-ii/docs/#14-classification-on-large-dataset","title":"1.4 Classification on Large Dataset\u00b6","text":"<p>To show a classification example on real world data, we will use a job placement dataset and try to predict how many college graduates got jobs immediately after graduation.</p>"},{"location":"03-model-building-ii/docs/#15-regression-using-trees","title":"1.5 Regression Using Trees\u00b6","text":"<p>Just like classification, trees can also predict numeric values using regression. Here we try to use trees to predict the housing prices in California using basic tree models.</p>"},{"location":"03-model-building-ii/docs/#2-random-forest-models","title":"2. Random Forest Models\u00b6","text":"<p>In random forest models, we train multiple decision trees on different subsets of the data and then have them vote on the correct answer. If it is a regression problem, we simply average out the results.</p>"},{"location":"03-model-building-ii/docs/#21-random-forest-hyper-parameters","title":"2.1 Random Forest Hyper-parameters\u00b6","text":"<p>Below are the main hyper-parameters you will need to choose when building a random forest model.</p> <ol> <li>n_estimators -&gt; The number of trees that will be used.</li> <li>max_depth -&gt; The maximum depth of each tree.</li> </ol> <p>We also put n_jobs = -1, this just tells the computer that it can use as much of the CPU as it wants to train our model very fast. If we want to use less of our CPU capacity we can set this to the maximum number of cores we want it to use.</p>"},{"location":"03-model-building-ii/docs/#22-randon-forest-classification","title":"2.2 Randon Forest Classification\u00b6","text":""},{"location":"03-model-building-ii/docs/#23-randon-forest-regression","title":"2.3 Randon Forest Regression\u00b6","text":"<p>We now apply random forest models to our regression dataset.</p>"},{"location":"03-model-building-ii/docs/#3-gradient-boosted-trees","title":"3. Gradient Boosted Trees\u00b6","text":"<p>Gradient boosted trees are models that utilise a chain of decision trees to make predictions. It generally follows these steps:</p> <ol> <li>Train an initial tree on the data.</li> <li>Gauge the overall error of this tree.</li> <li>Train a new tree that aims to rectify the mistakes of the prior tree instead of learning from scratch.</li> <li>Use the new tree to modify the answers of the previous tree</li> <li>Re-evaluate after the new tree corrects the answers.</li> <li>Repeat steps 3 - 5 for N number of trees.</li> <li>The final results is formed after (N - 1) trees correct each other\u2019s mistakes.</li> </ol> <p>For a fast and powerful implementation of such models we use the XGBoost library in these examples. Other popular trees include LGBM and ADA boost trees.</p>"},{"location":"03-model-building-ii/docs/#31-xgboost-and-the-main-hyper-parameters","title":"3.1 XGBoost and the main Hyper-parameters\u00b6","text":"<p>Xgboost is a more complex and efficient form of the normal gradient boosting algorithm. It is also optimised to run efficiently on both CPU and GPU hardware. Below are the main hyper-parameters you will need to choose when building an xgboost model.</p> <ol> <li>n_estimators -&gt; The number of trees that will be chained together.</li> <li>max_depth -&gt; The maximum depth of each tree in the chain.</li> </ol>"},{"location":"03-model-building-ii/docs/#32-xgboost-classification","title":"3.2 XGBoost Classification\u00b6","text":""},{"location":"03-model-building-ii/docs/#33-xgboost-regression","title":"3.3 XGBoost Regression\u00b6","text":""},{"location":"03-model-building-ii/docs/#4-comparing-performance-of-model-types","title":"4. Comparing Performance of Model Types\u00b6","text":""},{"location":"03-model-building-ii/docs/#41-classification","title":"4.1 Classification\u00b6","text":""},{"location":"03-model-building-ii/docs/#42-regression","title":"4.2 Regression\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/","title":"Evaluating and Tuning Models","text":"In\u00a0[15]: Copied! <pre>!git clone https://github.com/acmbpdc/ml-bootcamp-2023.git\n</pre> !git clone https://github.com/acmbpdc/ml-bootcamp-2023.git    <pre>fatal: destination path 'ml-bootcamp-2023' already exists and is not an empty directory.\n</pre> In\u00a0[16]: Copied! <pre>!cp /content/ml-bootcamp-2023/docs/04-evaluating-and-tuning/heart_v2.csv /content\n</pre> !cp /content/ml-bootcamp-2023/docs/04-evaluating-and-tuning/heart_v2.csv /content <p>When performing classification predictions, there's four types of outcomes that could occur.</p> <ul> <li>True Positives: When you predict an observation belongs to a class and it actually does belong to that class.</li> <li>True negatives: when you predict an observation does not belong to a class and it actually does not belong to that class.</li> <li>False positives: When you predict an observation belongs to a class when in reality it does not.</li> <li>False negatives: When you predict an observation does not belong to a class when in fact it does.</li> </ul> <p>These four outcomes are often plotted on a Confusion Matrix.</p> <p>Confusion Matrix: It is a performance measurement for machine learning classification problem where output can be two or more classes. It is a table with 4 different combinations of predicted and actual values.</p> <p>The following confusion matrix is an example for the case of binary classification. You would generate this matrix after making predictions on your test data and then identifying each prediction as one of the four possible outcomes described above. </p>  You can also extend this confusion matrix to plot multi-class classification predictions. The following is an example confusion matrix for classifying observations from the Iris flower dataset.  <p>The three main metrics used to evaluate a classification model are:</p> <ul> <li>Accuracy: The percentage of correct predictions for the test data. It can be calculated easily by dividing the number of correct predictions by the number of total      predictions.  Accuracy = (TP+TN)/Total</li> <li>Precision: The fraction of correct predictions (true positives) among all of the data points which were predicted to belong in a certain class.  Precision = (TP)/(TP+FP)</li> <li>Recall: The fraction of data points which were predicted to belong to a class with respect to all of the data points that truly belong in the class. Recall = (TP)/(TP+FN) </li> </ul> <p>Evaluation metrics for regression models are quite different than the above metrics we discussed for classification models because we are now predicting in a continuous range instead of a discrete number of classes </p> <ul> <li><p>Mean Squared Error:   or MSE for short, is a popular error metric for regression problems. It is also an important loss function for algorithms fit or optimized using the least squares framing of a regression problem. Here \u201cleast squares\u201d refers to minimizing the mean squared error between predictions and expected values. The MSE is calculated as the mean or average of the squared differences between predicted and expected target values in a dataset.</p> </li> <li><p>Root Mean Squared Error: or RMSE, is an extension of the mean squared error. Importantly, the square root of the error is calculated, which means that the units of the RMSE are the same as the original units of the target value that is being predicted. Thus, it may be common to use MSE loss to train a regression predictive model, and to use RMSE to evaluate and report its performance.</p> </li> </ul> <ul> <li>Mean Absolute Error:or MAE, is a popular metric because, like RMSE, the units of the error score match the units of the target value that is being predicted. Unlike the RMSE, the changes in MAE are linear and therefore intuitive.That is, MSE and RMSE punish larger errors more than smaller errors, inflating or magnifying the mean error score. This is due to the square of the error value. The MAE does not give more or less weight to different types of errors and instead the scores increase linearly with increases in error </li> </ul> In\u00a0[17]: Copied! <pre># Importing the required libraries\nimport pandas as pd, numpy as np\nimport matplotlib.pyplot as plt, seaborn as sns\n%matplotlib inline\n</pre> # Importing the required libraries import pandas as pd, numpy as np import matplotlib.pyplot as plt, seaborn as sns %matplotlib inline In\u00a0[18]: Copied! <pre># Reading the csv file and putting it into 'df' object.\ndf = pd.read_csv(\"/content/heart_v2.csv\")\n</pre> # Reading the csv file and putting it into 'df' object. df = pd.read_csv(\"/content/heart_v2.csv\") In\u00a0[19]: Copied! <pre>df.columns\n</pre> df.columns Out[19]: <pre>Index(['age', 'sex', 'BP', 'cholestrol', 'heart disease'], dtype='object')</pre> In\u00a0[20]: Copied! <pre>df.head()\n</pre> df.head() Out[20]: age sex BP cholestrol heart disease 0 70 1 130 322 1 1 67 0 115 564 0 2 57 1 124 261 1 3 64 1 128 263 0 4 74 0 120 269 0 In\u00a0[21]: Copied! <pre>df.shape\n</pre> df.shape Out[21]: <pre>(270, 5)</pre> In\u00a0[22]: Copied! <pre>df.info()\n</pre> df.info() <pre>&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 270 entries, 0 to 269\nData columns (total 5 columns):\n #   Column         Non-Null Count  Dtype\n---  ------         --------------  -----\n 0   age            270 non-null    int64\n 1   sex            270 non-null    int64\n 2   BP             270 non-null    int64\n 3   cholestrol     270 non-null    int64\n 4   heart disease  270 non-null    int64\ndtypes: int64(5)\nmemory usage: 10.7 KB\n</pre> In\u00a0[23]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.violinplot(df['age'])\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.violinplot(df['age']) plt.show() In\u00a0[24]: Copied! <pre>plt.figure(figsize = (15,5))\nax= sns.countplot(df['sex'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 45)\nplt.show()\n</pre> plt.figure(figsize = (15,5)) ax= sns.countplot(df['sex']) for p in ax.patches:     ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01)) plt.xticks(rotation = 45) plt.show()  In\u00a0[25]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.violinplot(df['BP'])\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.violinplot(df['BP']) plt.show() In\u00a0[26]: Copied! <pre>percentiles = df['BP'].quantile([0.05,0.95]).values\ndf['BP'][df['BP'] &lt;= percentiles[0]] = percentiles[0]\ndf['BP'][df['BP'] &gt;= percentiles[1]] = percentiles[1]\n</pre> percentiles = df['BP'].quantile([0.05,0.95]).values df['BP'][df['BP'] &lt;= percentiles[0]] = percentiles[0] df['BP'][df['BP'] &gt;= percentiles[1]] = percentiles[1] In\u00a0[27]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.violinplot(df['BP'])\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.violinplot(df['BP']) plt.show() In\u00a0[28]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.violinplot(df['cholestrol'])\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.violinplot(df['cholestrol']) plt.show() In\u00a0[29]: Copied! <pre>percentiles = df['cholestrol'].quantile([0.05,0.95]).values\ndf['cholestrol'][df['cholestrol'] &lt;= percentiles[0]] = percentiles[0]\ndf['cholestrol'][df['cholestrol'] &gt;= percentiles[1]] = percentiles[1]\n</pre> percentiles = df['cholestrol'].quantile([0.05,0.95]).values df['cholestrol'][df['cholestrol'] &lt;= percentiles[0]] = percentiles[0] df['cholestrol'][df['cholestrol'] &gt;= percentiles[1]] = percentiles[1] <pre>&lt;ipython-input-29-7853381cafe8&gt;:2: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['cholestrol'][df['cholestrol'] &lt;= percentiles[0]] = percentiles[0]\n&lt;ipython-input-29-7853381cafe8&gt;:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['cholestrol'][df['cholestrol'] &gt;= percentiles[1]] = percentiles[1]\n</pre> In\u00a0[30]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.violinplot(df['cholestrol'])\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.violinplot(df['cholestrol']) plt.show() In\u00a0[31]: Copied! <pre>plt.figure(figsize = (15,5))\nax= sns.countplot(df['heart disease'])\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 45)\nplt.show()\n</pre> plt.figure(figsize = (15,5)) ax= sns.countplot(df['heart disease']) for p in ax.patches:     ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01)) plt.xticks(rotation = 45) plt.show() In\u00a0[32]: Copied! <pre>plt.figure(figsize = (10,5))\nsns.violinplot(y = 'age', x = 'heart disease', data = df)\nplt.show()\n</pre> plt.figure(figsize = (10,5)) sns.violinplot(y = 'age', x = 'heart disease', data = df) plt.show() In\u00a0[33]: Copied! <pre>plt.figure(figsize = (10,5))\nax= sns.countplot(x = \"sex\", hue = \"heart disease\", data = df)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01))\nplt.xticks(rotation = 90)\nplt.show()\n</pre> plt.figure(figsize = (10,5)) ax= sns.countplot(x = \"sex\", hue = \"heart disease\", data = df) for p in ax.patches:     ax.annotate(str(p.get_height()), (p.get_x() * 1.01 , p.get_height() * 1.01)) plt.xticks(rotation = 90) plt.show() In\u00a0[34]: Copied! <pre>plt.figure(figsize = (10,5))\nsns.violinplot(y = 'BP', x = 'heart disease', data = df)\nplt.show()\n</pre> plt.figure(figsize = (10,5)) sns.violinplot(y = 'BP', x = 'heart disease', data = df) plt.show() In\u00a0[35]: Copied! <pre>plt.figure(figsize = (10,5))\nsns.violinplot(y = 'cholestrol', x = 'heart disease', data = df)\nplt.show()\n</pre> plt.figure(figsize = (10,5)) sns.violinplot(y = 'cholestrol', x = 'heart disease', data = df) plt.show() In\u00a0[36]: Copied! <pre>plt.figure(figsize = (10,5))\nsns.heatmap(df.corr(), annot = True, cmap=\"rainbow\")\nplt.show()\n</pre> plt.figure(figsize = (10,5)) sns.heatmap(df.corr(), annot = True, cmap=\"rainbow\") plt.show() In\u00a0[37]: Copied! <pre>df.describe()\n</pre> df.describe() Out[37]: age sex BP cholestrol heart disease count 270.000000 270.000000 270.000000 270.000000 270.000000 mean 54.433333 0.677778 130.824444 247.895185 0.444444 std 9.109067 0.468195 15.387319 42.641693 0.497827 min 29.000000 0.000000 106.900000 177.000000 0.000000 25% 48.000000 0.000000 120.000000 213.000000 0.000000 50% 55.000000 1.000000 130.000000 245.000000 0.000000 75% 61.000000 1.000000 140.000000 280.000000 1.000000 max 77.000000 1.000000 160.000000 326.550000 1.000000 In\u00a0[38]: Copied! <pre># Putting feature variable to X\nX = df.drop('heart disease',axis=1)\n\n# Putting response variable to y\ny = df['heart disease']\n</pre> # Putting feature variable to X X = df.drop('heart disease',axis=1)  # Putting response variable to y y = df['heart disease'] In\u00a0[39]: Copied! <pre>from sklearn.model_selection import train_test_split\n</pre> from sklearn.model_selection import train_test_split In\u00a0[40]: Copied! <pre>X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=50)\nX_train.shape, X_test.shape\n</pre> X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.6, random_state=50) X_train.shape, X_test.shape Out[40]: <pre>((162, 4), (108, 4))</pre> <p>Fitting the decision tree with default hyperparameters, apart from max_depth which is 3 so that we can plot and read the tree.</p> In\u00a0[41]: Copied! <pre>from sklearn.tree import DecisionTreeClassifier\n</pre> from sklearn.tree import DecisionTreeClassifier In\u00a0[42]: Copied! <pre>dt = DecisionTreeClassifier(max_depth=3)\ndt.fit(X_train, y_train)\n</pre> dt = DecisionTreeClassifier(max_depth=3) dt.fit(X_train, y_train) Out[42]: <pre>DecisionTreeClassifier(max_depth=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(max_depth=3)</pre> In\u00a0[43]: Copied! <pre>from sklearn import tree\nfig = plt.figure(figsize=(25,20))\n_ = tree.plot_tree(dt,\n                   feature_names=X.columns,\n                   class_names=['No Disease', \"Disease\"],\n                   filled=True)\n</pre> from sklearn import tree fig = plt.figure(figsize=(25,20)) _ = tree.plot_tree(dt,                    feature_names=X.columns,                    class_names=['No Disease', \"Disease\"],                    filled=True) In\u00a0[44]: Copied! <pre>y_train_pred = dt.predict(X_train)\ny_test_pred = dt.predict(X_test)\n</pre> y_train_pred = dt.predict(X_train) y_test_pred = dt.predict(X_test) In\u00a0[45]: Copied! <pre>from sklearn.metrics import confusion_matrix, accuracy_score\n</pre> from sklearn.metrics import confusion_matrix, accuracy_score In\u00a0[46]: Copied! <pre>print(accuracy_score(y_train, y_train_pred))\nconfusion_matrix(y_train, y_train_pred)\n</pre> print(accuracy_score(y_train, y_train_pred)) confusion_matrix(y_train, y_train_pred) <pre>0.7160493827160493\n</pre> Out[46]: <pre>array([[72, 15],\n       [31, 44]])</pre> In\u00a0[47]: Copied! <pre>print(accuracy_score(y_test, y_test_pred))\nconfusion_matrix(y_test, y_test_pred)\n</pre> print(accuracy_score(y_test, y_test_pred)) confusion_matrix(y_test, y_test_pred) <pre>0.7037037037037037\n</pre> Out[47]: <pre>array([[51, 12],\n       [20, 25]])</pre> <p>Creating helper functions to evaluate model performance and help plot the decision tree</p> In\u00a0[48]: Copied! <pre>def get_dt_graph(dt_classifier):\n    fig = plt.figure(figsize=(25,20))\n    _ = tree.plot_tree(dt_classifier,\n                       feature_names=X.columns,\n                       class_names=['No Disease', \"Disease\"],\n                       filled=True)\n</pre> def get_dt_graph(dt_classifier):     fig = plt.figure(figsize=(25,20))     _ = tree.plot_tree(dt_classifier,                        feature_names=X.columns,                        class_names=['No Disease', \"Disease\"],                        filled=True) In\u00a0[49]: Copied! <pre>def evaluate_model(dt_classifier):\n    print(\"Train Accuracy :\", accuracy_score(y_train, dt_classifier.predict(X_train)))\n    print(\"Train Confusion Matrix:\")\n    print(confusion_matrix(y_train, dt_classifier.predict(X_train)))\n    print(\"-\"*50)\n    print(\"Test Accuracy :\", accuracy_score(y_test, dt_classifier.predict(X_test)))\n    print(\"Test Confusion Matrix:\")\n    print(confusion_matrix(y_test, dt_classifier.predict(X_test)))\n</pre> def evaluate_model(dt_classifier):     print(\"Train Accuracy :\", accuracy_score(y_train, dt_classifier.predict(X_train)))     print(\"Train Confusion Matrix:\")     print(confusion_matrix(y_train, dt_classifier.predict(X_train)))     print(\"-\"*50)     print(\"Test Accuracy :\", accuracy_score(y_test, dt_classifier.predict(X_test)))     print(\"Test Confusion Matrix:\")     print(confusion_matrix(y_test, dt_classifier.predict(X_test))) In\u00a0[50]: Copied! <pre>dt_default = DecisionTreeClassifier(random_state=42)\ndt_default.fit(X_train, y_train)\n</pre> dt_default = DecisionTreeClassifier(random_state=42) dt_default.fit(X_train, y_train) Out[50]: <pre>DecisionTreeClassifier(random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(random_state=42)</pre> In\u00a0[51]: Copied! <pre>gph = get_dt_graph(dt_default)\n</pre> gph = get_dt_graph(dt_default)  In\u00a0[52]: Copied! <pre>evaluate_model(dt_default)\n</pre> evaluate_model(dt_default) <pre>Train Accuracy : 1.0\nTrain Confusion Matrix:\n[[87  0]\n [ 0 75]]\n--------------------------------------------------\nTest Accuracy : 0.7222222222222222\nTest Confusion Matrix:\n[[46 17]\n [13 32]]\n</pre> In\u00a0[53]: Copied! <pre>dt_depth = DecisionTreeClassifier(max_depth=3)\ndt_depth.fit(X_train, y_train)\n</pre> dt_depth = DecisionTreeClassifier(max_depth=3) dt_depth.fit(X_train, y_train) Out[53]: <pre>DecisionTreeClassifier(max_depth=3)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(max_depth=3)</pre> In\u00a0[54]: Copied! <pre>gph = get_dt_graph(dt_depth)\n</pre> gph = get_dt_graph(dt_depth)  In\u00a0[55]: Copied! <pre>evaluate_model(dt_depth)\n</pre> evaluate_model(dt_depth) <pre>Train Accuracy : 0.7160493827160493\nTrain Confusion Matrix:\n[[72 15]\n [31 44]]\n--------------------------------------------------\nTest Accuracy : 0.7037037037037037\nTest Confusion Matrix:\n[[51 12]\n [20 25]]\n</pre> In\u00a0[56]: Copied! <pre>dt_min_split = DecisionTreeClassifier(min_samples_split=20)\ndt_min_split.fit(X_train, y_train)\n</pre> dt_min_split = DecisionTreeClassifier(min_samples_split=20) dt_min_split.fit(X_train, y_train) Out[56]: <pre>DecisionTreeClassifier(min_samples_split=20)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(min_samples_split=20)</pre> In\u00a0[57]: Copied! <pre>gph = get_dt_graph(dt_min_split)\n</pre> gph = get_dt_graph(dt_min_split)  In\u00a0[58]: Copied! <pre>evaluate_model(dt_min_split)\n</pre> evaluate_model(dt_min_split) <pre>Train Accuracy : 0.7839506172839507\nTrain Confusion Matrix:\n[[68 19]\n [16 59]]\n--------------------------------------------------\nTest Accuracy : 0.7222222222222222\nTest Confusion Matrix:\n[[47 16]\n [14 31]]\n</pre> In\u00a0[59]: Copied! <pre>dt_min_leaf = DecisionTreeClassifier(min_samples_leaf=20, random_state=42)\ndt_min_leaf.fit(X_train, y_train)\n</pre> dt_min_leaf = DecisionTreeClassifier(min_samples_leaf=20, random_state=42) dt_min_leaf.fit(X_train, y_train) Out[59]: <pre>DecisionTreeClassifier(min_samples_leaf=20, random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(min_samples_leaf=20, random_state=42)</pre> In\u00a0[60]: Copied! <pre>gph = get_dt_graph(dt_min_leaf)\n</pre> gph = get_dt_graph(dt_min_leaf) In\u00a0[61]: Copied! <pre>evaluate_model(dt_min_leaf)\n</pre> evaluate_model(dt_min_leaf) <pre>Train Accuracy : 0.6728395061728395\nTrain Confusion Matrix:\n[[70 17]\n [36 39]]\n--------------------------------------------------\nTest Accuracy : 0.7037037037037037\nTest Confusion Matrix:\n[[53 10]\n [22 23]]\n</pre> In\u00a0[62]: Copied! <pre>dt_min_leaf_entropy = DecisionTreeClassifier(min_samples_leaf=20, random_state=42, criterion=\"entropy\")\ndt_min_leaf_entropy.fit(X_train, y_train)\n</pre> dt_min_leaf_entropy = DecisionTreeClassifier(min_samples_leaf=20, random_state=42, criterion=\"entropy\") dt_min_leaf_entropy.fit(X_train, y_train) Out[62]: <pre>DecisionTreeClassifier(criterion='entropy', min_samples_leaf=20,\n                       random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(criterion='entropy', min_samples_leaf=20,\n                       random_state=42)</pre> In\u00a0[63]: Copied! <pre>gph = get_dt_graph(dt_min_leaf_entropy)\n</pre> gph = get_dt_graph(dt_min_leaf_entropy) In\u00a0[64]: Copied! <pre>evaluate_model(dt_min_leaf_entropy)\n</pre> evaluate_model(dt_min_leaf_entropy) <pre>Train Accuracy : 0.6728395061728395\nTrain Confusion Matrix:\n[[70 17]\n [36 39]]\n--------------------------------------------------\nTest Accuracy : 0.7037037037037037\nTest Confusion Matrix:\n[[53 10]\n [22 23]]\n</pre> In\u00a0[65]: Copied! <pre>dt = DecisionTreeClassifier(random_state=42)\n</pre> dt = DecisionTreeClassifier(random_state=42) In\u00a0[66]: Copied! <pre>from sklearn.model_selection import GridSearchCV\n</pre> from sklearn.model_selection import GridSearchCV In\u00a0[67]: Copied! <pre># Create the parameter grid based on the results of random search \nparams = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}\n</pre> # Create the parameter grid based on the results of random search  params = {     'max_depth': [2, 3, 5, 10, 20],     'min_samples_leaf': [5, 10, 20, 50, 100],     'criterion': [\"gini\", \"entropy\"] } In\u00a0[68]: Copied! <pre># grid_search = GridSearchCV(estimator=dt, \n#                            param_grid=params, \n#                            cv=4, n_jobs=-1, verbose=1, scoring = \"f1\")\n</pre> # grid_search = GridSearchCV(estimator=dt,  #                            param_grid=params,  #                            cv=4, n_jobs=-1, verbose=1, scoring = \"f1\") In\u00a0[69]: Copied! <pre># Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=dt, \n                           param_grid=params, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")\n</pre> # Instantiate the grid search model grid_search = GridSearchCV(estimator=dt,                             param_grid=params,                             cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\") In\u00a0[70]: Copied! <pre>%%time\ngrid_search.fit(X_train, y_train)\n</pre> %%time grid_search.fit(X_train, y_train) <pre>Fitting 4 folds for each of 50 candidates, totalling 200 fits\nCPU times: user 210 ms, sys: 60.4 ms, total: 271 ms\nWall time: 3.25 s\n</pre> Out[70]: <pre>GridSearchCV(cv=4, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n             param_grid={'criterion': ['gini', 'entropy'],\n                         'max_depth': [2, 3, 5, 10, 20],\n                         'min_samples_leaf': [5, 10, 20, 50, 100]},\n             scoring='accuracy', verbose=1)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCV<pre>GridSearchCV(cv=4, estimator=DecisionTreeClassifier(random_state=42), n_jobs=-1,\n             param_grid={'criterion': ['gini', 'entropy'],\n                         'max_depth': [2, 3, 5, 10, 20],\n                         'min_samples_leaf': [5, 10, 20, 50, 100]},\n             scoring='accuracy', verbose=1)</pre>estimator: DecisionTreeClassifier<pre>DecisionTreeClassifier(random_state=42)</pre>DecisionTreeClassifier<pre>DecisionTreeClassifier(random_state=42)</pre> In\u00a0[71]: Copied! <pre>score_df = pd.DataFrame(grid_search.cv_results_)\nscore_df.head()\n</pre> score_df = pd.DataFrame(grid_search.cv_results_) score_df.head() Out[71]: mean_fit_time std_fit_time mean_score_time std_score_time param_criterion param_max_depth param_min_samples_leaf params split0_test_score split1_test_score split2_test_score split3_test_score mean_test_score std_test_score rank_test_score 0 0.009063 0.004237 0.006601 0.003827 gini 2 5 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.707317 0.463415 0.675 0.600 0.611433 0.093908 11 1 0.006161 0.003423 0.002779 0.000424 gini 2 10 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.707317 0.463415 0.650 0.600 0.605183 0.090229 13 2 0.005637 0.002345 0.005961 0.004609 gini 2 20 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.682927 0.463415 0.650 0.600 0.599085 0.083709 18 3 0.009143 0.002225 0.003528 0.001033 gini 2 50 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.658537 0.585366 0.700 0.700 0.660976 0.046820 1 4 0.005379 0.002786 0.002833 0.000740 gini 2 100 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.536585 0.536585 0.550 0.525 0.537043 0.008851 41 In\u00a0[72]: Copied! <pre>score_df.nlargest(5,\"mean_test_score\")\n</pre> score_df.nlargest(5,\"mean_test_score\") Out[72]: mean_fit_time std_fit_time mean_score_time std_score_time param_criterion param_max_depth param_min_samples_leaf params split0_test_score split1_test_score split2_test_score split3_test_score mean_test_score std_test_score rank_test_score 3 0.009143 0.002225 0.003528 0.001033 gini 2 50 {'criterion': 'gini', 'max_depth': 2, 'min_sam... 0.658537 0.585366 0.7 0.7 0.660976 0.04682 1 8 0.009332 0.003751 0.003004 0.000126 gini 3 50 {'criterion': 'gini', 'max_depth': 3, 'min_sam... 0.658537 0.585366 0.7 0.7 0.660976 0.04682 1 13 0.013625 0.003656 0.003548 0.002025 gini 5 50 {'criterion': 'gini', 'max_depth': 5, 'min_sam... 0.658537 0.585366 0.7 0.7 0.660976 0.04682 1 18 0.005282 0.002904 0.008118 0.003435 gini 10 50 {'criterion': 'gini', 'max_depth': 10, 'min_sa... 0.658537 0.585366 0.7 0.7 0.660976 0.04682 1 23 0.005860 0.004009 0.004118 0.003008 gini 20 50 {'criterion': 'gini', 'max_depth': 20, 'min_sa... 0.658537 0.585366 0.7 0.7 0.660976 0.04682 1 In\u00a0[73]: Copied! <pre>grid_search.best_estimator_\n</pre> grid_search.best_estimator_ Out[73]: <pre>DecisionTreeClassifier(max_depth=2, min_samples_leaf=50, random_state=42)</pre>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifier<pre>DecisionTreeClassifier(max_depth=2, min_samples_leaf=50, random_state=42)</pre> In\u00a0[74]: Copied! <pre>dt_best = grid_search.best_estimator_\n</pre> dt_best = grid_search.best_estimator_ In\u00a0[75]: Copied! <pre>evaluate_model(dt_best)\n</pre> evaluate_model(dt_best) <pre>Train Accuracy : 0.6604938271604939\nTrain Confusion Matrix:\n[[54 33]\n [22 53]]\n--------------------------------------------------\nTest Accuracy : 0.6018518518518519\nTest Confusion Matrix:\n[[38 25]\n [18 27]]\n</pre> In\u00a0[76]: Copied! <pre>get_dt_graph(dt_best)\n</pre> get_dt_graph(dt_best) In\u00a0[77]: Copied! <pre>from sklearn.metrics import classification_report\n</pre> from sklearn.metrics import classification_report In\u00a0[80]: Copied! <pre>print(classification_report(dt_best.predict(X_train), y_train))\n</pre> print(classification_report(dt_best.predict(X_train), y_train)) <pre>              precision    recall  f1-score   support\n\n           0       0.62      0.71      0.66        76\n           1       0.71      0.62      0.66        86\n\n    accuracy                           0.66       162\n   macro avg       0.66      0.66      0.66       162\nweighted avg       0.67      0.66      0.66       162\n\n</pre> In\u00a0[81]: Copied! <pre>print(classification_report(dt_best.predict(X_test), y_test))\n</pre> print(classification_report(dt_best.predict(X_test), y_test)) <pre>              precision    recall  f1-score   support\n\n           0       0.60      0.68      0.64        56\n           1       0.60      0.52      0.56        52\n\n    accuracy                           0.60       108\n   macro avg       0.60      0.60      0.60       108\nweighted avg       0.60      0.60      0.60       108\n\n</pre>"},{"location":"04-evaluating-and-tuning/docs/#evaluating-and-tuning-models","title":"Evaluating and Tuning Models\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#summary","title":"Summary:\u00b6","text":"<ul> <li>Evaluating the performance of models</li> <li>Metrics to measure the performance of models</li> <li>Hyperparameter tuning of the model, to achieve maximum accuracy</li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#metrics-to-evaluate-machine-learning-models","title":"Metrics to Evaluate Machine Learning models\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#the-traintestvalidation-split","title":"The Train/Test/Validation split\u00b6","text":"<p>The most important thing you can do to properly evaluate your model is to not train the model on the entire dataset. A typical train/test/validation split would be to use 60% of the data for training set, 20% of the data for test set, and 20% of the data for validation set.</p> <ul> <li><p>Train Set: This subset of the dataset is used for training the parameters of your machine learning model, and repetitively iterating and improving the model weights. The accuracy of the model on this subset of dataset is called Train Accuracy.</p> </li> <li><p>Test Set: This subset of the dataset is used for checking the accuracy of the model on new, unseen data while the training process is ongoing. The accuracy of the model on this subset is called Test Accuracy, and is used for tuning the hyperparameters (learning rate, batch size, number of epochs, optimizer, etc), to improve the accuracy.</p> </li> <li><p>Validation Accuracy: (optional) Now, since we have tuned the hyperparameters to the Test Set to give good accuracy, but maybe the hyperparameters chosen don't work for another new set of data. So, as a final check we can also use another subset of the dataset called Validation Set, to check if the model is generalising well.</p> </li> </ul> <p>Note: In case you have a small dataset and do not want to make a separate validation set (as that would mean lesser data for training), you can use train accuracy and test accuracy, if the train and test accuracy are within 4-5% of each other, it means that the model is working well for new data.</p>"},{"location":"04-evaluating-and-tuning/docs/#classification-metrics","title":"Classification Metrics\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#why-do-we-use-precision-and-recall","title":"Why do we use Precision and Recall?\u00b6","text":"<ul> <li>Precision and recall are useful in cases where classes aren't evenly distributed.</li> <li>The common example is for developing a classification algorithm that predicts whether or not someone has a disease. If only a small percentage of the population (let's say 1%) has this disease, we could build a classifier that always predicts that the person does not have the disease, we would have built a model which is 99% accurate and 0% useful.</li> <li>If we had checked the recall of the model described above, this could have been easily diagnosed.</li> <li>Recall ensures that we're not overlooking the people who have the disease, while precision ensures that we're not misclassifying too many people as having the disease when they don't.</li> </ul> <p>A metric called F1 Score is used to represent Precision and recall together.  F1 Score = Precision*Recall/ (Precision+Recall)</p>"},{"location":"04-evaluating-and-tuning/docs/#regression-metrics","title":"Regression Metrics\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#overfitting-vs-underfitting-bias-vs-variance","title":"Overfitting vs Underfitting (Bias vs Variance)\u00b6","text":"<ul> <li>Overfitting (Bias) refers to a model that models the training data too well. Overfitting happens when a model learns the detail and noise in the training data to the extent that it negatively impacts the performance of the model on new data. This means that the noise or random fluctuations in the training data is picked up and learned as concepts by the model. The problem is that these concepts do not apply to new data and negatively impact the models ability to generalize. Signs of Overfitting: Your train accuracy is high, but your test accuracy is much lower</li> <li>Underfitting refers to a model that can neither model the training data nor generalize to new data. An underfit machine learning model is not a suitable model and will be obvious as it will have poor performance on the training data. Underfitting is often not discussed as it is easy to detect given a good performance metric. The remedy is to move on and try alternate machine learning algorithms Signs of Underfitting: Both your train and test accuracy are low</li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#why-does-overfitting-occur","title":"Why does overfitting occur?:\u00b6","text":"<p>Overfitting happens due to several reasons, such as:</p> <ul> <li>The training data size is too small and does not contain enough data samples to accurately represent all possible input data values.</li> <li>The training data contains large amounts of irrelevant information, called noisy data.</li> <li>The model trains for too long on a single sample set of data.</li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#strategies-to-overcome-overfitting","title":"Strategies to overcome overfitting\u00b6","text":"<p>You can prevent overfitting by diversifying and scaling your training data set or using some other data science strategies, like those given below.</p> <ul> <li><p>Early stopping Early stopping pauses the training phase before the machine learning model learns the noise in the data. However, getting the timing right is important; else the model will still not give accurate results.</p> </li> <li><p>Pruning You might identify several features or parameters that impact the final prediction when you build a model. Feature selection\u2014or pruning\u2014identifies the most important features within the training set and eliminates irrelevant ones</p> </li> <li><p>Regularization Regularization is a collection of training/optimization techniques that seek to reduce overfitting. These methods try to eliminate those factors that do not impact the prediction outcomes by grading features based on importance</p> </li> <li><p>Ensembling Ensembling combines predictions from several separate machine learning algorithms. Some models are called weak learners because their results are often inaccurate. Ensemble methods combine all the weak learners to get more accurate results</p> </li> <li><p>Data augmentation Data augmentation is a machine learning technique that changes the sample data slightly every time the model processes it. You can do this by changing the input data in small ways. When done in moderation, data augmentation makes the training sets appear unique to the model and prevents the model from learning their characteristics</p> </li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#hyperparameter-tuning","title":"Hyperparameter Tuning\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#what-are-hyperparameters","title":"What are Hyperparameters?\u00b6","text":"<ul> <li>Hyperparameters are the parameters that cannot be estimated by the model itself and you need you specify them manually. These parameters affect your model in many ways for a specific set of data.</li> <li>For your model to perform best you have choose the best set of hyperparameters.</li> <li>For example, Max Depth in Decision Tree and Learning rate in Deep Neural Network. These hyperparameters have a direct effect on whether your model will be Underfitting or Overfitting. The Bias Variance trade-off is heavily reliant on hyperparameter tuning.</li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#what-is-hyperparameter-tuning","title":"What is Hyperparameter Tuning?\u00b6","text":"<p>In most of the ML models there will be multiple hyperparameters. Choosing the best combination requires an understanding of the model's parameters and the business problem you\u2019re trying to tackle. So before doing anything you have to know the hyperparameters of the models and their importance.</p> <p>To get the best hyperparameter the two step procedure is followed .</p> <ul> <li>For each combination of hyperparameters the model is evaluated</li> <li>The combination that gives the best performing model are selected as optimal hyperparameters.</li> </ul> <p>So, put simply, choosing the best set of hyperparameters that result in the best model is called Hyperparameter Tuning.</p>"},{"location":"04-evaluating-and-tuning/docs/#methods-for-tuning-hyperparameters","title":"Methods for Tuning Hyperparameters\u00b6","text":"<p>Now that we understand what hyperparameters are and the importance of tuning them, we need to know how to choose their optimal values. We can find these optimal hyperparameter values using manual or automated methods.</p> <p>When tuning hyperparameters manually, we typically start using the default recommended values or rules of thumb, then search through a range of values using trial-and-error. But manual tuning is a tedious and time-consuming approach. It isn\u2019t practical when there are many hyperparameters with a wide range.</p> <p>Automated hyperparameter tuning methods use an algorithm to search for the optimal values. Some of today\u2019s most popular automated methods are grid search, random search, and Bayesian optimization. Let\u2019s explore these methods in detail.</p> <ul> <li>Grid Search:<ul> <li>Grid search is a sort of \u201cbrute force\u201d hyperparameter tuning method. We create a grid of possible discrete hyperparameter values then fit the model with every possible combination. We record the model performance for each set then select the combination that has produced the best performance.</li> <li>Grid search is an exhaustive algorithm that can find the best combination of hyperparameters. However, the drawback is that it\u2019s slow. Fitting the model with every possible combination usually requires a high computation capacity and significant time</li> </ul> </li> </ul> <ul> <li>Random search:<ul> <li>The random search method (as its name implies) chooses values randomly rather than using a predefined set of values like the grid search method.</li> <li>Random search tries a random combination of hyperparameters in each iteration and records the model performance. After several iterations, it returns the mix that produced the best result.</li> <li>Random search is appropriate when we have several hyperparameters with relatively large search domains.</li> <li>The benefit is that random search typically requires less time than grid search to return a comparable result, but the drawback is that the result may not be the best possible hyperparameter combination.</li> </ul> </li> </ul>"},{"location":"04-evaluating-and-tuning/docs/#hyperparameter-tuning-example-for-decision-trees","title":"Hyperparameter tuning example for Decision Trees\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#eda","title":"EDA\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#evaluating-model-performance","title":"Evaluating model performance\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#without-setting-any-hyper-parameters","title":"Without setting any hyper-parameters\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#controlling-the-depth-of-the-tree","title":"Controlling the depth of the tree\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#specifying-minimum-samples-before-split","title":"Specifying minimum samples before split\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#specifying-minimum-samples-in-leaf-node","title":"Specifying minimum samples in leaf node\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#using-entropy-instead-of-gini","title":"Using Entropy instead of Gini\u00b6","text":""},{"location":"04-evaluating-and-tuning/docs/#hyper-parameter-tuning","title":"Hyper-parameter tuning\u00b6","text":""}]}